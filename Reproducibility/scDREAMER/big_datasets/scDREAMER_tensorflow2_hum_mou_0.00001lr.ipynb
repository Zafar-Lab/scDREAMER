{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ba6OBfX1gnzG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 19:58:57.279877: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-12 19:58:58.070935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ajita/anaconda3/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "542964"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "#import tables\n",
    "import scipy.io\n",
    "from sklearn.decomposition import PCA\n",
    "import pdb\n",
    "\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy import stats \n",
    "from scipy import * \n",
    "import datetime \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf2\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()\n",
    "np.random.seed(0)\n",
    "#tf.set_random_seed(0)\n",
    "tf.set_random_seed(0)\n",
    "random.seed(0)\n",
    "tf2.random.set_seed(0)\n",
    "tf2.keras.utils.set_random_seed(0)\n",
    "#tf.keras.utils.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fkVerHg8IS97"
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "'''\n",
    "To automate this: to be set by User ; dataset_name, category: To automate later\n",
    "Lung:16\n",
    "Simulation: sim_full 6 (for simulation 1; latent matrix will be saved in sim_full folder)\n",
    "Simulation2: 4\n",
    "Pan:9 # new pancreas data used\n",
    "Pancreas_: 4\n",
    "Immune: 10\n",
    "Immune_mouse: Immune 23\n",
    "MouseBrain: 4, # does not work\n",
    "Tabula: 2\n",
    "hum_mou: 2\n",
    "'''\n",
    "\n",
    "dataset_name = 'hum_mou'#'Simulation' # 'Pancreas' # 'Lung', Pancreas2 #sim_full, Immune, sim2, simulation2, Simulation2, \"Pan\", Tabula\n",
    "category = 2 #6 for simulation # 3 for pancreas, 16 for lung, 10 for Immune, 6, 4 Simulation2, 9 for Pan, 2 for Tabula\n",
    "hvg = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mYXBjJYgqqc",
    "outputId": "3dad9292-28a8-4135-f22c-3eae1a9b4919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Go9Bo6ceSDoO",
    "outputId": "1ea07214-720c-4489-a9c6-89a94d62efb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install -U scipy==1.5.0\\n!pip install scanpy\\n!pip install -U scikit-learn\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\"\"\"\n",
    "!pip install -U scipy==1.5.0\n",
    "!pip install scanpy\n",
    "!pip install -U scikit-learn\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qyIXAPC_VJ0h"
   },
   "outputs": [],
   "source": [
    "#path = '/content/drive/My Drive/Colab Notebooks/Project/advVAE/'\n",
    "#path = '/home/ajita/Documents/IITK_AS/DI/SCRNA_Datasets/'\n",
    "path = \"/home/ajita/Documents/data_integration/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i_Uk1Y5JXU0O"
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "# Util.py .....\n",
    "def read_mtx(filename, trans_flag = False):\n",
    "     buffer = scipy.io.mmread(filename)\n",
    "\n",
    "     if trans_flag:\n",
    "         print('Transpose Data !')\n",
    "         return buffer.transpose()\n",
    "     else:\n",
    "         return buffer\n",
    "\n",
    "def write_mtx(filename, data):\n",
    "    scipy.io.mmwrite(filename, data)\n",
    "\n",
    "# Leaky Relu\n",
    "def lrelu(x, alpha = 0.2, name='lrelu'):\n",
    "    return tf.maximum(x, alpha*x)\n",
    "\n",
    "def dense(x, inp_dim, out_dim, name = 'dense'):\n",
    "\n",
    "    with tf.variable_scope(name, reuse=None): # earlier only tf\n",
    "        weights = tf.get_variable(\"weights\", shape=[inp_dim, out_dim],\n",
    "                                  initializer = tf2.initializers.GlorotUniform()) # contrib: tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        bias = tf.get_variable(\"bias\", shape=[out_dim], initializer = tf.constant_initializer(0.0))\n",
    "        \n",
    "        # initializer= tf2.initializers.GlorotUniform(); same as Xavier's initializer; tf.contrib.layers.xavier_initializer()    \n",
    "        out = tf.add(tf.matmul(x, weights), bias, name='matmul')\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cOjEeW9VCzK0"
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_npy():\n",
    "    batch_info = np.load(path + 'Pancreas_/accessions.npy')\n",
    "    features = np.load(path + 'Pancreas_/features.npy')\n",
    "    labels = np.load(path + 'Pancreas_/labels.npy')\n",
    "\n",
    "    batch_info = np.array([[i] for i in batch_info])\n",
    "\n",
    "    Ann = sc.AnnData(pd.DataFrame(features))\n",
    "    sc.pp.log1p(Ann)\n",
    "\n",
    "    sc.pp.highly_variable_genes(\n",
    "        Ann, \n",
    "        flavor=\"seurat\", \n",
    "        n_top_genes = 2000, # 721\n",
    "        subset=True)\n",
    "\n",
    "    df_final = pd.DataFrame(Ann.X).reset_index(drop = True)\n",
    "    data = df_final[df_final.columns[:2000]].to_numpy()\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_h5ad(data_path, B, C):\n",
    "\n",
    "    Ann = sc.read_h5ad(data_path)\n",
    "    Ann.layers[\"counts\"] = Ann.X.copy()\n",
    "    sc.pp.normalize_total(Ann, target_sum=1e4)\n",
    "    sc.pp.log1p(Ann)\n",
    "    Ann.raw = Ann \n",
    "    \n",
    "    if (dataset_name == 'Pancreas_'):\n",
    "      Ann.obs[B] = Ann.obsm[B].astype('str')\n",
    "      Ann.obs[C] = Ann.obsm[C].astype('str')\n",
    "\n",
    "    sc.pp.highly_variable_genes(\n",
    "          Ann, \n",
    "          flavor=\"seurat\", \n",
    "          n_top_genes=hvg,\n",
    "          batch_key=B,\n",
    "          subset=True)\n",
    "  \n",
    "    if (dataset_name == \"Pan\" or dataset_name == \"Immune\" or dataset_name == \"Pancreas_\"):\n",
    "      df_final = pd.DataFrame(Ann.X) # Immune , Pan\n",
    "\n",
    "    else:\n",
    "      df_final = pd.DataFrame.sparse.from_spmatrix(Ann.X) # Lung, Simulation1, Simulation 2\n",
    "\n",
    "    print (\"data shape\", df_final.shape)\n",
    "    df_final = df_final.reset_index(drop = True)\n",
    "    data = df_final.to_numpy()\n",
    "    labels = Ann.obs[C].to_list()\n",
    "\n",
    "    t_ = Ann.obs[B] #.to_list()\n",
    "    batch_info = np.array([[i] for i in t_]) # for other datasets\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')  \n",
    "    enc.fit(batch_info.reshape(-1, 1))\n",
    "    batch_info_enc = enc.transform(batch_info.reshape(-1, 1)).toarray()\n",
    "\n",
    "    return data, labels, batch_info_enc, batch_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "d7jU6FDDXhtJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_gene_mtx(dataset_name, transform = True, count = True, actv = 'sig'):\n",
    "\n",
    "    if (dataset_name == 'Lung'):\n",
    "      data_name = 'Lung/Lung_atlas_public.h5ad'\n",
    "      B = 'batch'\n",
    "      C = 'cell_type'\n",
    "\n",
    "    elif (dataset_name == 'Immune'):\n",
    "      data_name = 'Immune/Immune_ALL_hum_mou.h5ad' #'Immune/Immune_ALL_human.h5ad' \n",
    "      B = 'batch'\n",
    "      C = 'final_annotation'\n",
    "\n",
    "    elif (dataset_name == 'sim_full'):\n",
    "      data_name = 'Simulation/sim1_full.h5ad' \n",
    "      B = 'Batch'\n",
    "      C = 'Group' \n",
    "      \n",
    "    elif (dataset_name == 'Simulation2'):\n",
    "      data_name = 'Simulation2/sim2.h5ad' \n",
    "      B = 'Batch'\n",
    "      C = 'Group'\n",
    "\n",
    "    elif (dataset_name == \"Pan\"):\n",
    "      data_name = \"Pan/Pancreas.h5ad\"\n",
    "      B = \"tech\"\n",
    "      C = \"celltype\"\n",
    "\n",
    "    elif (dataset_name == 'Pancreas_'):\n",
    "      data_name = 'Pancreas_/pancreas_scDGN.h5ad'\n",
    "      B = 'batch'\n",
    "      C = 'cell_type' \n",
    "\n",
    "    elif(dataset_name == 'MouseBrain'):\n",
    "      data_name = 'MouseBrain'\n",
    "      data_name = \"MouseBrain/Mouse_Brain.h5\"\n",
    "      B = 'batch'\n",
    "      C = 'tissue'\n",
    "\n",
    "    elif(dataset_name == 'Tabula'):\n",
    "      data_name = \"Tabula/tab_muris_senis_merged_v2.h5ad\"\n",
    "      B = 'batch'\n",
    "      C = 'cell'\n",
    "\n",
    "    elif(dataset_name == 'hum_mou'):\n",
    "      data_name = \"hum_mou/hcl_mca_merged.h5ad\"\n",
    "      B = 'batch'\n",
    "      C = 'celltype'\n",
    "\n",
    "    data, labels, batch_info_enc, batch_info = read_h5ad(path + data_name, B, C)\n",
    "         \n",
    "    print ('Shape of data is: ', data.shape)\n",
    "    \n",
    "    '''\n",
    "    if transform:\n",
    "        print (\"inside if\")\n",
    "        data = transform_01_gene_input(data)\n",
    "        print('Data Transformed, entries in [0, 1] !'.format(data_path))\n",
    "    else:\n",
    "        print (\"inside else\")\n",
    "        if count == False:\n",
    "            data = np.log2(data+1)\n",
    "\n",
    "            if actv == 'lin':\n",
    "                scale = 1.0\n",
    "            else:\n",
    "                scale = np.max(data)\n",
    "            data = data / scale           \n",
    "    '''\n",
    "    ord_enc = LabelEncoder()\n",
    "    labels  = ord_enc.fit_transform(labels)\n",
    "    print ('here', labels)\n",
    "\n",
    "    unique, counts = np.unique(labels, return_counts = True)\n",
    "    dict(zip(unique, counts))\n",
    "    \n",
    "    total_size = data.shape[0]\n",
    "\n",
    "    #if count == False:\n",
    "    #    return data, data, scale, labels, labels, batch_info_enc, batch_info_enc, batch_info, B, C\n",
    "\n",
    "    return data, data, labels, labels, labels, batch_info_enc, batch_info_enc, batch_info, B, C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8MDNCdZC9Qe"
   },
   "source": [
    "Class **Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnC_XfBeHHq-"
   },
   "source": [
    "Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UMmPH2fy_G1j"
   },
   "outputs": [],
   "source": [
    "# Class Functions:\n",
    "def build_model(self):\n",
    "\n",
    "    self.N_batch = category\n",
    "    self.x_input = tf.placeholder(dtype=tf.float32, shape=[None, self.X_dim], name='Input')\n",
    "    self.x_input_ = tf.placeholder(dtype=tf.float32, shape=[None, self.X_dim], name='Input')\n",
    "    self.x_target = tf.placeholder(dtype=tf.float32, shape=[None, self.X_dim], name='Target')\n",
    "    self.batch_input = tf.placeholder(dtype = tf.float32, shape=[None, self.N_batch], name='batch_input') # 6, 3\n",
    "    self.batch_input_ = tf.placeholder(dtype = tf.float32, shape=[None, self.N_batch], name='batch_input')\n",
    "    \n",
    "    self.keep_prob = tf.placeholder(dtype=tf.float32, name = 'keep_prob')\n",
    "    self.real_distribution = tf.placeholder(dtype=tf.float32, shape=[None, self.z_dim], name='Real_distribution')\n",
    "    self.kl_scale = tf.placeholder(tf.float32, (), name='kl_scale')\n",
    "    \n",
    "    self.kl_scale = 0.001 # 0.01 later..., 0.001 for human mouse and in 2109 file\n",
    "    self.dropout_rate = 0.1\n",
    "    self.training_phase = True \n",
    "    self.n_layers = self.num_layers \n",
    "    self.n_latent = self.z_dim\n",
    "    \n",
    "    # AJ\n",
    "    self.enc_input = tf.concat([self.x_input, self.batch_input],1)\n",
    "    self.enc_input_ = tf.concat([self.x_input_, self.batch_input_],1)\n",
    "    print('encoder input shape ',self.enc_input)\n",
    "\n",
    "    # AJ: Encoder output...\n",
    "    self.encoder_output, self.z_post_m, self.z_post_v, self.l_post_m, self.l_post_v = self.encoder(self.enc_input) # self.x_input\n",
    "    self.encoder_output_, self.z_post_m_, self.z_post_v_, self.l_post_m_, self.l_post_v_ = self.encoder(self.enc_input_, reuse = True) \n",
    "\n",
    "    self.expression = self.x_input               \n",
    "    self.proj = tf.placeholder(dtype=tf.float32, shape=[None, self.X_dim], name='projection')\n",
    "  \n",
    "    log_library_size = np.log(np.sum(self.data_train, axis=1)) \n",
    "    mean, variance = np.mean(log_library_size), np.var(log_library_size)\n",
    "\n",
    "    library_size_mean = mean\n",
    "    library_size_variance = variance\n",
    "    self.library_size_mean = tf.to_float(tf.constant(library_size_mean))\n",
    "    self.library_size_variance = tf.to_float(tf.constant(library_size_variance))    \n",
    "    \n",
    "    self.z = self.sample_gaussian(self.z_post_m, self.z_post_v) \n",
    "    self.z_ = self.sample_gaussian(self.z_post_m_, self.z_post_v_) \n",
    "\n",
    "    self.library = self.sample_gaussian(self.l_post_m, self.l_post_v)\n",
    "    \n",
    "    # AJ\n",
    "    #self.decoder_output = self.decoder(self.z)  \n",
    "    print('decoder input shape ',tf.concat([self.z, self.batch_input],1))\n",
    "\n",
    "    self.decoder_output = self.decoder(tf.concat([self.z, self.batch_input],1))             \n",
    "    self.n_input = self.expression.get_shape().as_list()[1]\n",
    "  \n",
    "    self.x_post_scale = tf.nn.softmax(dense(self.decoder_output, self.g_h_dim[0], self.n_input, name='dec_x_post_scale')) \n",
    "    self.x_post_r = tf.Variable(tf.random_normal([self.n_input]), name=\"dec_x_post_r\")           \n",
    "    self.x_post_rate = tf.exp(self.library) * self.x_post_scale\n",
    "    self.x_post_dropout = dense(self.decoder_output, self.g_h_dim[0], self.n_input, name='dec_x_post_dropout') \n",
    "        \n",
    "    local_dispersion = tf.exp(self.x_post_r)            \n",
    "    local_l_mean = self.library_size_mean\n",
    "    local_l_variance = self.library_size_variance\n",
    "\n",
    "    self.decoder_output2 = tf.nn.sigmoid(dense(self.decoder_output, self.g_h_dim[0], self.X_dim, 'dec_output2'))  \n",
    "    \n",
    "    # Discriminator D1....\n",
    "    self.dis_real_logit = self.discriminator(self.real_distribution, self.z_dim) # random z from Gaussina distribution ...\n",
    "    self.dis_fake_logit = self.discriminator(self.z, self.z_dim, reuse=True)  # z distribution coming from encoder...network \n",
    "    \n",
    "    # Discriminator D2\n",
    "    self.dis2_real_logit = self.discriminator2(self.x_target, self.X_dim) # True data\n",
    "    self.dis2_fake_logit = self.discriminator2(self.decoder_output2, self.X_dim, reuse=True) # from decoder network\n",
    "    \n",
    "    # Discriminator D_batch : discriminate between different batches info\n",
    "    # pass the encoded data; self.x_target, self.X_dim\n",
    "    self.disb_real_logit = self.discriminatorB(self.z, self.z_dim) # True data\n",
    "    \n",
    "    # Reconstruction loss \n",
    "    capL = 1e-4 #1e-8\n",
    "    capU = 1e4 #1e8\n",
    "\n",
    "    #recon_loss = 0\n",
    "    recon_loss = self.zinb_model(self.expression, self.x_post_rate, local_dispersion, self.x_post_dropout)\n",
    "    #recon_loss = tf.reduce_mean(tf.square(tf.subtract( self.decoder_output2, self.x_input)))\n",
    "    \n",
    "    self.kl_gauss_l = 0.5 * tf.reduce_sum(- tf.log(tf.math.minimum(tf.math.maximum(self.l_post_v, capL), capU))  \\\n",
    "                                      + self.l_post_v/local_l_variance \\\n",
    "                                      + tf.square(self.l_post_m - local_l_mean)/local_l_variance  \\\n",
    "                                      + tf.log(tf.math.minimum(tf.math.maximum(local_l_variance, capL), capU)) - 1, 1)\n",
    "\n",
    "    self.kl_gauss_z = 0.5 * tf.reduce_sum(- tf.log(tf.math.minimum(tf.math.maximum(self.z_post_v, capL), capU)) + self.z_post_v + tf.square(self.z_post_m) - 1, 1)\n",
    "\n",
    "    print ('KL gaussian z', self.kl_gauss_z)\n",
    "    print ('KL gaussian l', self.kl_gauss_l)\n",
    "\n",
    "    # Evidence lower bound - ELBO : KLscale to prevent posterior collapse...\n",
    "    #self.ELBO_gauss = tf.reduce_mean(recon_loss - self.kl_gauss_l - self.kl_scale * self.kl_gauss_z) - tf.reduce_sum(tf.pow(self.z - self.z_, 2)) \n",
    "    self.ELBO_gauss = tf.reduce_mean(recon_loss - self.kl_scale*self.kl_gauss_l - self.kl_scale*self.kl_gauss_z) #- tf.reduce_sum(tf.pow(self.z - self.z_, 2)) \n",
    "\n",
    "    #tf.reduce_sum(tf.math.sqrt(self.z - self.z_)) #tf.reduce_sum(tf.pow(self.z - self.z_, 2))\n",
    "    \n",
    "    # - is added to ELBO because we maximize the ELBO expression & maximize classifier cross entropy loss -> -loss minimize cross entropy\n",
    "    self.autoencoder_loss = - self.ELBO_gauss - tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.disb_real_logit, labels = self.batch_input))  - tf.log(tf.math.minimum(tf.math.maximum(tf.reduce_sum(tf.sqrt(tf.abs(self.dis2_real_logit/tf.reduce_sum(self.dis2_real_logit)\n",
    "                                        * self.dis2_fake_logit/tf.reduce_sum(self.dis2_fake_logit)))), capL), capU))                     \n",
    "          \n",
    "    # Discriminator D1: minimize distance   min max objective function - BD distance between z (random sample) and generated z sample from encoder\n",
    "    self.dis_loss = - tf.log(tf.math.minimum(tf.math.maximum(tf.reduce_sum(tf.sqrt(tf.abs(self.dis_real_logit/tf.reduce_sum(self.dis_real_logit)\n",
    "                                        * self.dis_fake_logit/tf.reduce_sum(self.dis_fake_logit)))), capL), capU)) \n",
    "            \n",
    "    # Discriminator D2: minimize distance min max objective function - BD distance between X(generated sample) and X input \n",
    "    self.dis2_loss = tf.log(tf.math.minimum(tf.math.maximum(tf.reduce_sum(tf.sqrt(tf.abs(self.dis2_real_logit/tf.reduce_sum(self.dis2_real_logit)\n",
    "                                        * self.dis2_fake_logit/tf.reduce_sum(self.dis2_fake_logit)))), capL), capU)) # epsilon added to avoid Nan\n",
    "    \n",
    "    # Generator loss - D(z) {D(E(X_real))}  - D1 label will be 1; they are trying to maximize the \n",
    "    self.generator_loss = - tf.log(tf.math.minimum(tf.math.maximum(tf.reduce_sum(tf.sqrt(tf.abs(self.dis_fake_logit/tf.reduce_sum(self.dis_fake_logit)) )), capL), capU)) \n",
    "\n",
    "            \n",
    "    # 27Apr: AJ : minimize binary cross entropy     \n",
    "    self.disb_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.disb_real_logit, labels = self.batch_input)) #self.batch_input\n",
    "    \n",
    "    t_vars = tf.trainable_variables()\n",
    "    self.dis_vars = [var for var in t_vars if 'dis_' in var.name]\n",
    "    self.gen_vars = [var for var in t_vars if 'enc_' in var.name or 'dec_' in var.name] #AS:2109\n",
    "\n",
    "    # Discriminator D2\n",
    "    self.dis2_vars = [var for var in t_vars if 'dis2_' in var.name]\n",
    "    \n",
    "    # Discriminator DB: AJ\n",
    "    self.disb_vars = [var for var in t_vars if 'disb_' in var.name]\n",
    "\n",
    "    self.saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJ0gZUTxHJnd"
   },
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eAL6VTFuF1cD"
   },
   "outputs": [],
   "source": [
    "def train_cluster(self):\n",
    "\n",
    "    print('Cluster DRA on DataSet {} ... '.format(self.dataset_name))\n",
    "\n",
    "    #tf.train.Optimizer\n",
    "    #autoencoder_optimizer = tf.keras.optimizers.SGD(learning_rate=0.1).minimize(self.autoencoder_loss, var_list = self.gen_vars)\n",
    "    #autoencoder_optimizer = tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.autoencoder_loss)\n",
    "\n",
    "    # learning_rate=self.lr 0.0001,beta1=self.beta1 #0.0002, 0.001\n",
    "    autoencoder_optimizer = tf.train.AdamOptimizer(learning_rate=0.00001,beta1=self.beta1).minimize(self.autoencoder_loss) # self.disb_vars, var_list = self.gen_vars\n",
    " \n",
    "    # Not used at the moment....\n",
    "    #autoencoder_optimizer2 = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "    #                                               beta1=self.beta1).minimize( self.autoencoder_loss) # self.disb_vars\n",
    "\n",
    "    discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "                                                      beta1=self.beta1).minimize(self.dis_loss,\n",
    "                                                                                  var_list=self.dis_vars)\n",
    "    generator_optimizer = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "                                                  beta1=self.beta1).minimize(self.generator_loss,var_list=self.gen_vars)\n",
    "    # Discriminator D2\n",
    "    discriminator2_optimizer = tf.train.AdamOptimizer(learning_rate=0.00001,\n",
    "                                                      beta1=self.beta1).minimize(self.dis2_loss,\n",
    "                                                                                  var_list=self.dis2_vars) # removed self.lr\n",
    "    \n",
    "    # Discriminator batch: Classifier....\n",
    "    \n",
    "    discriminatorb_optimizer = tf.train.AdamOptimizer(learning_rate = self.lr,\n",
    "                                              beta1 = self.beta1).minimize(self.disb_loss, var_list = self.disb_vars)\n",
    "    \n",
    "    self.sess.run(tf.global_variables_initializer())\n",
    "    a_loss_epoch = []\n",
    "    d_loss_epoch = []\n",
    "    g_loss_epoch = []\n",
    "    d2_loss_epoch = [] # Discriminator D2\n",
    "    db_loss_epoch = [] # Discriminator batch\n",
    "\n",
    "    control = 3 # Generator is updated twice for each Discriminator D1 update\n",
    "    \n",
    "    print (\"Training begins...\")\n",
    "    num_batch_iter = self.total_size // self.batch_size\n",
    "    indices = np.arange(self.data_train.shape[0])\n",
    "    \n",
    "    for ep in range(self.epoch):\n",
    "    #for it in range(num_batch_iter):\n",
    "    \n",
    "        d_loss_curr = g_loss_curr = a_loss_curr = np.inf\n",
    "        self._is_train = True\n",
    "\n",
    "        #index = 0\n",
    "        #np.random.shuffle(indices)\n",
    "        \n",
    "        for it in range(num_batch_iter):\n",
    "\n",
    "            # Selecting mini batch\n",
    "            \"\"\"\n",
    "            batch_indices = indices[index : index + self.batch_size]\n",
    "            \n",
    "            batch_x = self.data_train[batch_indices, :]\n",
    "            X_ = self.batch_train[batch_indices, :]\n",
    "            labels_ = self.labels_enc[batch_indices, :]\n",
    "            labels_n = self.labels_na[batch_indices]            \n",
    "            index += self.batch_size\n",
    "            \"\"\"\n",
    "            batch_x, X_ = self.next_batch(self.data_train, self.batch_train, self.train_size)\n",
    "\n",
    "            #df = pd.DataFrame(batch_x)\n",
    "            #col = df.columns[:100] # random col generate\n",
    "            #for c in col: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "            #    df[c] = 0\n",
    "            \n",
    "            #batch_x_ = df.to_numpy()\n",
    "\n",
    "            # Random Sampling for the batch size...\n",
    "            batch_z_real_dist = self.sample_Z(self.batch_size, self.z_dim)\n",
    "\n",
    "            #print (\"...hi hi....\")\n",
    "            _, a_loss_curr = self.sess.run([autoencoder_optimizer, self.autoencoder_loss],\n",
    "                                            feed_dict={self.x_input: batch_x, self.x_target: batch_x, \n",
    "                                                       #self.x_input_: batch_x_, self.batch_input_: X_,\n",
    "                                                      self.batch_input: X_, # batch_b\n",
    "                                                      self.keep_prob: self.keep_param}) \n",
    "  \n",
    "                \n",
    "            #if np.mod(it, control) == 0: \n",
    "\n",
    "            '''  \n",
    "            _, d_loss_curr = self.sess.run([discriminator_optimizer, self.dis_loss],\n",
    "                feed_dict={self.x_input: batch_x,\n",
    "                            self.batch_input: X_,\n",
    "                self.real_distribution: batch_z_real_dist,\n",
    "                #self.x_input_: batch_x_, self.batch_input_: X_,\n",
    "                self.keep_prob: self.keep_param})    \n",
    "\n",
    "                                                      \n",
    "            else: \n",
    "                \n",
    "                _, g_loss_curr = self.sess.run([generator_optimizer, self.generator_loss], \n",
    "                    feed_dict={self.x_input: batch_x, self.x_target: batch_x, self.keep_prob: self.keep_param, \n",
    "                               self.x_input_: batch_x_, self.batch_input_: X_,\n",
    "                               self.batch_input: X_, self.real_distribution: batch_z_real_dist}) #self.generator_loss\n",
    "            '''\n",
    "            # AJ: Count here, D2 is taking true data only.\n",
    "            _, d2_loss_curr = self.sess.run([discriminator2_optimizer, self.dis2_loss],\n",
    "                        feed_dict={self.x_input: batch_x,\n",
    "                        self.x_target: batch_x,\n",
    "                        self.batch_input: X_,\n",
    "                        self.real_distribution: batch_z_real_dist,\n",
    "                        #self.x_input_: batch_x_, self.batch_input_: X_,\n",
    "                        self.keep_prob: self.keep_param}) \n",
    "            \n",
    "            _, db_loss_curr = self.sess.run([discriminatorb_optimizer, self.disb_loss],\n",
    "                feed_dict={self.x_input: batch_x,\n",
    "                            self.batch_input: X_,\n",
    "                self.real_distribution: batch_z_real_dist,\n",
    "                self.batch_input: X_, # batch_b\n",
    "                #self.x_input_: batch_x_, self.batch_input_: X_,\n",
    "                self.keep_prob: self.keep_param})\n",
    "            \n",
    "            \n",
    "        #db_loss_curr = 0\n",
    "        print(\"Epoch : [%d] ,  a_loss = %.4f, d_loss: %.4f ,  g_loss: %.4f,  db_loss: %.4f\" \n",
    "              % (ep, a_loss_curr, d_loss_curr, g_loss_curr,db_loss_curr))\n",
    "        \n",
    "        \n",
    "        self._is_train = False # enables false after 1st iterations only...to make training process fast\n",
    "\n",
    "        if (np.isnan(a_loss_curr) or np.isnan(d_loss_curr) or np.isnan(g_loss_curr) or np.isnan(db_loss_curr)): # np.isnan(d2_loss_curr)\n",
    "          a_loss_curr = 0\n",
    "          d_loss_curr = 0\n",
    "          g_loss_curr = 0\n",
    "          d2_loss_curr = 0\n",
    "          db_loss_curr = 0\n",
    "          break\n",
    "\n",
    "        #self.x_target: batch_x,\n",
    "        a_loss_epoch.append(a_loss_curr) # total loss getting appended \n",
    "        d_loss_epoch.append(d_loss_curr)\n",
    "        g_loss_epoch.append(g_loss_curr)\n",
    "        d2_loss_epoch.append(d2_loss_curr)\n",
    "        db_loss_epoch.append(db_loss_curr)\n",
    "        \n",
    "        if (ep % 50 == 0): #and ep != 0\n",
    "            self.eval_cluster_on_test_(ep)\n",
    "\n",
    "    self.eval_cluster_on_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_t3LUqLHLz_"
   },
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XYHxxwi7GL2V"
   },
   "outputs": [],
   "source": [
    "# reuse = False\n",
    "\n",
    "def encoder(self, x, reuse = False):\n",
    "    \"\"\"\n",
    "    Encode part of the autoencoder.\n",
    "    :param x: input to the autoencoder\n",
    "    :param reuse: True -> Reuse the encoder variables, False -> Create or search of variables before creating\n",
    "    :return: tensor which is the hidden latent variable of the autoencoder.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope('Encoder') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        if self.is_bn:\n",
    "            h = tf.layers.batch_normalization(\n",
    "\n",
    "                lrelu(dense(x, self.X_dim + self.N_batch, self.g_h_dim[0], name='enc_h0_lin'), alpha=self.leak),\n",
    "                training=self._is_train, name='enc_bn0')\n",
    "                \n",
    "            for i in range(1, self.num_layers):\n",
    "                h = tf.layers.batch_normalization(\n",
    "\n",
    "                    lrelu(dense(h, self.g_h_dim[i - 1], self.g_h_dim[i], name='enc_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak),\n",
    "                    training=self._is_train, name='enc_bn' + str(i))                    \n",
    "\n",
    "            z_post_m = dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_z_post_m' + str(self.num_layers) + '_lin')                \n",
    "            z_post_v = tf.exp(dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_z_post_v' + str(self.num_layers) + '_lin'))              \n",
    "            \n",
    "            h = tf.nn.relu(dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_h' + str(self.num_layers) + '_lin'))\n",
    "\n",
    "            l_post_m = dense(h, self.z_dim, 1, name='enc_l_post_m' + str(self.num_layers) + '_lin')                            \n",
    "            l_post_v = tf.exp(dense(h, self.z_dim, 1, name='enc_l_post_v' + str(self.num_layers) + '_lin')) \n",
    "            \n",
    "\n",
    "        else:\n",
    "\n",
    "            h = tf.nn.dropout(lrelu(dense(x, self.X_dim + self.N_batch, self.g_h_dim[0], name='enc_h0_lin'), alpha=self.leak),\n",
    "                              keep_prob=self.keep_prob)                \n",
    "            \n",
    "            for i in range(1, self.num_layers):\n",
    "                \n",
    "                h = tf.nn.dropout(lrelu(dense(h, self.g_h_dim[i - 1], self.g_h_dim[i], name='enc_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak), keep_prob=self.keep_prob)                    \n",
    "\n",
    "            z_post_m = dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_z_post_m' + str(self.num_layers) + '_lin')                \n",
    "            z_post_v = tf.exp(dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_z_post_v' + str(self.num_layers) + '_lin'))\n",
    "            \n",
    "            \n",
    "            h = tf.nn.relu(dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_h' + str(self.num_layers) + '_lin'))\n",
    "                      \n",
    "\n",
    "            l_post_m = dense(h, self.z_dim, 1, name='enc_l_post_m' + str(self.num_layers) + '_lin')                             \n",
    "            l_post_v = tf.exp(dense(h, self.z_dim, 1, name='enc_l_post_v' + str(self.num_layers) + '_lin'))                                          \n",
    "                        \n",
    "        return h, z_post_m, z_post_v, l_post_m, l_post_v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXs3ma44HNlC"
   },
   "source": [
    "Discriminator - Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "I3agMa0XG4nW"
   },
   "outputs": [],
   "source": [
    "def discriminator(self, z, z_dim, reuse=False):    \n",
    "    \"\"\"\n",
    "    Discriminator that is used to match the posterior distribution with a given prior distribution.\n",
    "    :param z: tensor of shape [batch_size, z_dim]\n",
    "    :param reuse: True -> Reuse the discriminator variables,\n",
    "                  False -> Create or search of variables before creating\n",
    "    :return: tensor of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Discriminator') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        if self.is_bn:\n",
    "\n",
    "            h = tf.layers.batch_normalization(\n",
    "                lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='dis_h' + str(self.num_layers-1) + '_lin'),      \n",
    "                      alpha=self.leak),\n",
    "                training=self._is_train, name='dis_bn' + str(self.num_layers-1))\n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                h = tf.layers.batch_normalization(\n",
    "                    lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='dis_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak),\n",
    "                    training=self._is_train, name='dis_bn' + str(i))\n",
    "\n",
    "        else:\n",
    "\n",
    "            h = tf.nn.dropout(\n",
    "                lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='dis_h' + str(self.num_layers-1) + '_lin'),\n",
    "                      alpha=self.leak),\n",
    "                keep_prob=self.keep_prob)\n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                h = tf.nn.dropout(\n",
    "                    lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='dis_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak), keep_prob=self.keep_prob)\n",
    "\n",
    "        output = dense(h, self.d_h_dim[0], 1, name='dis_output')\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWo-DCr1HRsG"
   },
   "source": [
    "Discriminator - X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Vh_WY-GjHC2s"
   },
   "outputs": [],
   "source": [
    "def discriminator2(self, z, z_dim, reuse=False):    \n",
    "    \"\"\"\n",
    "    Discriminator that is used to match the posterior distribution with a given prior distribution.\n",
    "    :param z: tensor of shape [batch_size, z_dim]\n",
    "    :param reuse: True -> Reuse the discriminator variables,\n",
    "                  False -> Create or search of variables before creating\n",
    "    :return: tensor of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Discriminator2') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        if self.is_bn:\n",
    "\n",
    "            h = tf.layers.batch_normalization(\n",
    "                lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='dis2_h' + str(self.num_layers-1) + '_lin'),      \n",
    "                      alpha=self.leak),\n",
    "                training=self._is_train, name='dis2_bn' + str(self.num_layers-1))\n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                h = tf.layers.batch_normalization(\n",
    "                    lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='dis2_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak),\n",
    "                    training=self._is_train, name='dis2_bn' + str(i))\n",
    "\n",
    "        else:\n",
    "\n",
    "            h = tf.nn.dropout(\n",
    "                lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='dis2_h' + str(self.num_layers-1) + '_lin'),\n",
    "                      alpha=self.leak),\n",
    "                keep_prob=self.keep_prob)\n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                h = tf.nn.dropout(\n",
    "                    lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='dis2_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak), keep_prob=self.keep_prob)\n",
    "\n",
    "        output = dense(h, self.d_h_dim[0], 1, name='dis2_output')\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTzTzd0CHUxH"
   },
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bTTvlUYpGzoM"
   },
   "outputs": [],
   "source": [
    "def decoder(self, z, reuse=False):\n",
    "    \"\"\"\n",
    "    Decoder part of the autoencoder.\n",
    "    :param z: input to the decoder\n",
    "    :param reuse: True -> Reuse the decoder variables, False -> Create or search of variables before creating\n",
    "    :return: tensor which should ideally be the input given to the encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope('Decoder') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        if self.is_bn:\n",
    "\n",
    "            h = tf.layers.batch_normalization(\n",
    "              \n",
    "                lrelu(dense(z , self.z_dim + self.N_batch, self.g_h_dim[self.num_layers-1], name='dec_h' + str(self.num_layers-1) + '_lin'),\n",
    "                      alpha=self.leak),                   \n",
    "                training=self._is_train, name='dec_bn' + str(self.num_layers-1))\n",
    "            for i in range(self.num_layers-2, -1,-1):\n",
    "                h = tf.layers.batch_normalization(\n",
    "\n",
    "                    lrelu(dense(h, self.g_h_dim[i + 1], self.g_h_dim[i], name='dec_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak),                        \n",
    "                    training=self._is_train, name='dec_bn' + str(i))\n",
    "        else:\n",
    "            h = tf.nn.dropout(lrelu(dense(z, self.z_dim + self.N_batch, self.g_h_dim[self.num_layers-1], name='dec_h' + str(self.num_layers-1) + '_lin'),\n",
    "                                    alpha=self.leak),                                  \n",
    "                              keep_prob=self.keep_prob)\n",
    "            for i in range(self.num_layers-2, -1, -1):\n",
    "                h = tf.nn.dropout(\n",
    "                    lrelu(dense(h, self.g_h_dim[i + 1], self.g_h_dim[i], name='dec_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak), keep_prob=self.keep_prob)\n",
    "            \n",
    "        return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnxjzgINHWP5"
   },
   "source": [
    "Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0nuIBTqaG1Rl"
   },
   "outputs": [],
   "source": [
    "def discriminatorB(self, z, z_dim, reuse = False):    \n",
    "    \n",
    "    \"\"\"\n",
    "    Discriminator takes the real data and try ti differentiate between different batches\n",
    "    :param x: tensor of shape [batch_size, x_dim]\n",
    "    :param batch: tensor of shape [batch_size] -> batchinfo of the train data\n",
    "    :param reuse: True -> Reuse the discriminator variables,False -> Create or search of variables before creating\n",
    "    :return: tensor of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    '''\n",
    "    x_ = pd.DataFrame(x.numpy())\n",
    "    x_ = pd.concat([x_, batch], axis = 1)\n",
    "    x = torch.tensor(x.values)\n",
    "    x_dim = torch.tensor(721)\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope('discriminatorB') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        if self.is_bn:\n",
    "\n",
    "            h = tf.layers.batch_normalization(\n",
    "                lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='disb_h' + \n",
    "                            str(self.num_layers-1) + '_lin'), alpha = self.leak),\n",
    "                            training=self._is_train, name='disb_bn' + str(self.num_layers-1))\n",
    "            \n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                h = tf.layers.batch_normalization(\n",
    "                    lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='disb_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak),\n",
    "                    training=self._is_train, name='disb_bn' + str(i))\n",
    "\n",
    "        else:\n",
    "\n",
    "            h = tf.nn.dropout(\n",
    "                lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='disb_h' + str(self.num_layers-1) + '_lin'),\n",
    "                      alpha=self.leak),\n",
    "                keep_prob=self.keep_prob)\n",
    "            \n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                h = tf.nn.dropout(\n",
    "                    lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='disb_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak), keep_prob=self.keep_prob)\n",
    "\n",
    "        # AJ 6 in pace of 1 , 3 for pancreas \n",
    "        output = dense(h, self.d_h_dim[0], self.N_batch, name='disb_output')\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "w3bSCL1cHqFI"
   },
   "outputs": [],
   "source": [
    "#AJ: 20 may\n",
    "# Zero-inflated negative binomial (ZINB) model is for modeling count variables with excessive zeros and it is usually for overdispersed count outcome variables.\n",
    "def zinb_model(self, x, mean, inverse_dispersion, logit, eps=1e-4): \n",
    "\n",
    "    # 1e8 should be of same dimensions as other parameters....                 \n",
    "    expr_non_zero = - tf.nn.softplus(- logit) \\\n",
    "                    + tf.log(inverse_dispersion + eps) * inverse_dispersion \\\n",
    "                    - tf.log(inverse_dispersion + mean + eps) * inverse_dispersion \\\n",
    "                    - x * tf.log(inverse_dispersion + mean + eps) \\\n",
    "                    + x * tf.log(mean + eps) \\\n",
    "                    - tf.lgamma(x + 1) \\\n",
    "                    + tf.lgamma(x + inverse_dispersion) \\\n",
    "                    - tf.lgamma(inverse_dispersion) \\\n",
    "                    - logit \n",
    "    \n",
    "    expr_zero = - tf.nn.softplus( - logit) \\\n",
    "                + tf.nn.softplus(- logit + tf.log(inverse_dispersion + eps) * inverse_dispersion \\\n",
    "                                  - tf.log(inverse_dispersion + mean + eps) * inverse_dispersion) \n",
    "\n",
    "    template = tf.cast(tf.less(x, eps), tf.float32)\n",
    "    expr =  tf.multiply(template, expr_zero) + tf.multiply(1 - template, expr_non_zero)\n",
    "    return tf.reduce_sum(expr, axis=-1)\n",
    "\n",
    "def eval_cluster_on_test_(self, n):\n",
    "\n",
    "    # Embedding points in the test data to the latent space\n",
    "    inp_encoder = self.data_test\n",
    "    labels = self.labels_test\n",
    "    batch_label = self.batch_test\n",
    "            \n",
    "    #latent_matrix = self.sess.run(self.z, feed_dict = {self.x_input: inp_encoder, self.batch_input: batch_label, self.keep_prob: 1.0})\n",
    "\n",
    "    start = 0\n",
    "    end = 15000 # size of each pass\n",
    "    latent_matrix = self.sess.run(self.z, feed_dict = {self.x_input: inp_encoder[start:end], self.batch_input: batch_label[start:end], self.keep_prob: 1.0})\n",
    "\n",
    "    while (end < len(inp_encoder)):\n",
    "\n",
    "      #print (\"hi\")\n",
    "      start = end\n",
    "      end = min(end + 15000, len(inp_encoder))\n",
    "\n",
    "      mat = self.sess.run(self.z, feed_dict = {self.x_input: inp_encoder[start:end], self.batch_input: batch_label[start:end], self.keep_prob: 1.0})\n",
    "      latent_matrix = np.concatenate((latent_matrix, mat), axis = 0)\n",
    "\n",
    "      #print (end)\n",
    "\n",
    "\n",
    "    print ('latent_matrix shape', latent_matrix.shape)\n",
    "    print (labels.shape)\n",
    "    \n",
    "    np.savetxt(path + self.dataset_name + '/latent_matrix_c'+ str(n)+ '.csv', latent_matrix, delimiter=\",\")\n",
    "\n",
    "    ### comment from here....\n",
    "    '''\n",
    "    print (\"K means...\")\n",
    "    K = np.size(np.unique(labels))   \n",
    "    kmeans = KMeans(n_clusters=K, random_state=0).fit(latent_matrix)\n",
    "    y_pred = kmeans.labels_\n",
    "\n",
    "    print('Computing NMI ...')\n",
    "    NMI = nmi(labels.flatten(), y_pred.flatten())\n",
    "    print('Done !')\n",
    "\n",
    "    print('NMI = {}'. \n",
    "          format(NMI)) \n",
    "    \n",
    "    # Plotting....\n",
    "    #print (\"1\")\n",
    "    Ann = sc.AnnData(inp_encoder)\n",
    "    #print (\"2\")\n",
    "    Ann.obsm['final_embeddings'] = latent_matrix\n",
    "    Ann.obs['group'] = labels.astype(str)\n",
    "    \n",
    "    #print (\"3\")\n",
    "    sc.pp.neighbors(Ann, use_rep = 'final_embeddings') #use_rep = 'final_embeddings'\n",
    "    #print (\"4\")\n",
    "    sc.tl.umap(Ann)\n",
    "    img = sc.pl.umap(Ann, color = 'group', frameon = False) # cells 3 B, C #self.C\n",
    "    print(img)\n",
    "\n",
    "    #print (\"5\")\n",
    "    Ann.obs['batch'] = self.batch_info.astype(str)\n",
    "    #print (\"6\")\n",
    "    img2 = sc.pl.umap(Ann, color = 'batch', frameon = False) #self.B\n",
    "    #print (\"7\")\n",
    "    print(img2)\n",
    "    '''\n",
    "\n",
    "    \n",
    "def eval_cluster_on_test(self):\n",
    "\n",
    "    # Embedding points in the test data to the latent space\n",
    "    inp_encoder = self.data_test\n",
    "    labels = self.labels_test\n",
    "    batch_label = self.batch_test\n",
    "            \n",
    "    #latent_matrix = self.sess.run(self.z, feed_dict = {self.x_input: inp_encoder, self.batch_input: batch_label, self.keep_prob: 1.0})\n",
    "    \n",
    "    start = 0\n",
    "    end = 15000 # size of each pass\n",
    "    latent_matrix = self.sess.run(self.z, feed_dict = {self.x_input: inp_encoder[start:end], self.batch_input: batch_label[start:end], self.keep_prob: 1.0})\n",
    "\n",
    "    while (end < len(inp_encoder)):\n",
    "\n",
    "      #print (\"hi\")\n",
    "      start = end\n",
    "      end = min(end + 15000, len(inp_encoder))\n",
    "\n",
    "      mat = self.sess.run(self.z, feed_dict = {self.x_input: inp_encoder[start:end], self.batch_input: batch_label[start:end], self.keep_prob: 1.0})\n",
    "      latent_matrix = np.concatenate((latent_matrix, mat), axis = 0)\n",
    "      \n",
    "      #print (end)\n",
    "\n",
    "\n",
    "    print ('latent_matrix shape', latent_matrix.shape)\n",
    "    print (labels.shape)\n",
    "    \n",
    "    np.savetxt(path + self.dataset_name + '/latent_matrix_c.csv', latent_matrix, delimiter=\",\")\n",
    "\n",
    "    ### comment from here....\n",
    "    '''\n",
    "    Ann = sc.AnnData(inp_encoder)\n",
    "    Ann.obsm['final_embeddings'] = latent_matrix\n",
    "    Ann.obs['group'] = labels.astype(str)\n",
    "    \n",
    "    sc.pp.neighbors(Ann, use_rep = 'final_embeddings') #use_rep = 'final_embeddings'\n",
    "    sc.tl.umap(Ann)\n",
    "    img = sc.pl.umap(Ann, color = 'group', frameon = False) # cells\n",
    "    print(img)\n",
    "      \n",
    "    Ann.obs['batch'] = self.batch_info.astype(str)\n",
    "    img2 = sc.pl.umap(Ann, color = 'batch', frameon = False)\n",
    "    print(img2)\n",
    "\n",
    "    K = np.size(np.unique(labels))   \n",
    "    kmeans = KMeans(n_clusters=K, random_state=0).fit(latent_matrix)\n",
    "    y_pred = kmeans.labels_\n",
    "\n",
    "    print('Computing NMI ...')\n",
    "    NMI = nmi(labels.flatten(), y_pred.flatten())\n",
    "    print('Done !')\n",
    "\n",
    "    print('NMI = {}'. \n",
    "          format(NMI)) \n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "p9t6iZZZYSw4"
   },
   "outputs": [],
   "source": [
    "# DRA.py\n",
    "\n",
    "class Test_DRA(object):\n",
    "    def __init__(self, sess, epoch = 200, lr=0.0001, beta1=0.5, batch_size=128, X_dim=720, z_dim=10, dataset_name='mnist',\n",
    "                 checkpoint_dir='checkpoint', sample_dir='samples', result_dir = 'result', num_layers = 2, g_h_dim = None,\n",
    "                 d_h_dim = None, gen_activation = 'sig', leak = 0.2, keep_param = 1.0, trans = 'sparse',is_bn = False,\n",
    "                 g_iter = 2, lam=10.0, sampler = 'uniform'):\n",
    "\n",
    "        self.sess = sess\n",
    "        self.epoch = epoch\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.batch_size = batch_size\n",
    "        self.X_dim = X_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.dataset_name = dataset_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.sample_dir = sample_dir\n",
    "        self.result_dir = result_dir\n",
    "        self.num_layers = num_layers\n",
    "        self.g_h_dim = g_h_dim  # Fully connected layers for Generator\n",
    "        self.d_h_dim = d_h_dim  # Fully connected layers for Discriminator\n",
    "        self.gen_activation = gen_activation\n",
    "        self.leak = leak\n",
    "        self.keep_param = keep_param\n",
    "        self.trans = trans\n",
    "        self.is_bn = is_bn\n",
    "        self.g_iter = g_iter\n",
    "        self.lam = lam\n",
    "        self.sampler = sampler\n",
    "        self.eps = 0.001\n",
    "        self._is_train = False\n",
    "        self.n_hidden = 128 \n",
    "        \n",
    "        if self.trans == 'sparse':\n",
    "            self.data_train, self.data_test, self.scale, self.labels_train, self.labels_test, self.batch_train, self.batch_test, self.batch_info, self.B, self.C  = load_gene_mtx(self.dataset_name, transform=False, count=False, actv=self.gen_activation)\n",
    "            self.N_batch = self.batch_train.shape[1]\n",
    "        else:\n",
    "            self.data_train, self.data_test, self.labels_train, self.labels_val, self.labels_test, self.B, self.C  = load_gene_mtx(self.dataset_name, transform=True)\n",
    "            self.scale = 1.0\n",
    "                \n",
    "        if self.gen_activation == 'tanh':\n",
    "            self.data = 2* self.data - 1\n",
    "            self.data_train = 2 * self.data_train - 1\n",
    "            self.data_val = 2 * self.data_val - 1\n",
    "\n",
    "        print ('Data set to work on:')\n",
    "        print (self.data_train)\n",
    "        print (self.data_train.shape)\n",
    "        print (self.batch_train)\n",
    "        print (self.batch_train.shape)\n",
    "        \n",
    "        self.train_size = self.data_train.shape[0]\n",
    "        self.test_size = self.data_test.shape[0]\n",
    "        self.total_size = self.test_size\n",
    "    \n",
    "        self.build_model()\n",
    "\n",
    "    build_model = build_model\n",
    "    train_cluster = train_cluster\n",
    "    encoder = encoder\n",
    "    decoder = decoder\n",
    "    discriminatorB = discriminatorB\n",
    "    discriminator2 = discriminator2\n",
    "    discriminator = discriminator\n",
    "    eval_cluster_on_test = eval_cluster_on_test\n",
    "    eval_cluster_on_test_ = eval_cluster_on_test_\n",
    "    zinb_model = zinb_model\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        s = \"DRA_{}_{}_b_{}_g{}_d{}_{}_{}_lr_{}_b1_{}_leak_{}_keep_{}_z_{}_{}_bn_{}_lam_{}_giter_{}_epoch_{}\".format(\n",
    "            datetime.datetime.now(), self.dataset_name, \n",
    "            self.batch_size, self.g_h_dim, self.d_h_dim, self.gen_activation, self.trans, self.lr, \n",
    "            self.beta1, self.leak, self.keep_param, self.z_dim, self.sampler, self.is_bn,\n",
    "            self.lam, self.g_iter, self.epoch) \n",
    "        s = s.replace('[', '_')\n",
    "        s = s.replace(']', '_')\n",
    "        s = s.replace(' ', '')\n",
    "        return s\n",
    "\n",
    "    def sample_Z(self, m, n, sampler='uniform'):\n",
    "        if self.sampler == 'uniform':\n",
    "            return np.random.uniform(-1., 1., size=[m, n])\n",
    "        elif self.sampler == 'normal':\n",
    "            return np.random.randn(m, n)\n",
    "\n",
    "    def next_batch(self, data, batch_info, max_size):\n",
    "\n",
    "        indx = np.random.randint(max_size - self.batch_size)\n",
    "        return data[indx:(indx + self.batch_size), :], batch_info[indx:(indx + self.batch_size), :]\n",
    "        \n",
    "\n",
    "    def next_batch_(self, data, max_size):\n",
    "        #data = data.sample(frac = 1)\n",
    "        indx = np.random.randint(max_size - self.batch_size)\n",
    "        return data[indx:(indx + self.batch_size), :]\n",
    "\n",
    "    def sample_gaussian(self, mean, variance, scope=None):\n",
    "\n",
    "        with tf.variable_scope(scope, 'sample_gaussian'):\n",
    "            sample = tf.random_normal(tf.shape(mean), mean, tf.sqrt(variance))\n",
    "            sample.set_shape(mean.get_shape())\n",
    "            return sample\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KBGsSdFINaS7"
   },
   "outputs": [],
   "source": [
    "\n",
    "#python dra.py --model dra --batch_size 128 --learning_rate 0.0007 --beta1 0.9 --n_l 1 \n",
    "#--g_h_l1 512 --d_h_l1 512 --bn False --actv sig --trans sparse --keep 0.9 --leak 0.2 --lam 1.0 --epoch 900 --z_dim 10 --train --dataset Simulation\n",
    "\n",
    "learning_rate = 0.0007 # AJ: 18 may, decrease to address nan issue #0.0007\n",
    "epoch = 400 # \"Epoch to train [25]\") # ideal 200 - 2000 v.bary\n",
    "beta1 = 0.9 # \"Momentum term of adam [0.9]\")\n",
    "batch_size = 256 #\"The size of batch images [128]\") # 128 worked for all, 64, 256 is also option - hummou\n",
    "z_dim = 10 # \"Latent space dimension\")\n",
    "n_l = 1 # \"# Hidden Layers\")\n",
    "g_h_l1 = 512 #\"#Generator Hidden Units in Layer 1\")\n",
    "g_h_l2 = 256 # \"#Generator Hidden Units in Layer 2\")\n",
    "g_h_l3 = 0 # \"#Generator Hidden Units in Layer 3\")\n",
    "g_h_l4 = 0# \"#Generator Hidden Units in Layer 4\")\n",
    "d_h_l1 = 512 # \"#Discriminator Hidden Units in Layer 1\")\n",
    "d_h_l2 = 256 # \"#Discriminator Hidden Units in Layer 2\")\n",
    "d_h_l3 = 0 #\"#Discriminator Hidden Units in Layer 3\")\n",
    "d_h_l4 = 0 #Discriminator Hidden Units in Layer 4\")\n",
    "actv = \"sig\" # \"Decoder Activation [sig, tanh, lin]\")\n",
    "leak = 0.2 # \"Leak factor\")\n",
    "keep = 0.9 # \"Keep prob\")\n",
    "trans =  \"sparse\" # \"Data Transformation [dense, sparse]\")\n",
    "dataset = dataset_name # e.g. \"Simulation\" # \"The name of dataset [mnist, 10x_73k, 10x_68k, Zeisel, Macosko]\")\n",
    "checkpoint_dir = \"/data/eugene/AAE-20180306-Hemberg/test_checkpoint\" #\"Directory name to save the checkpoints [checkpoint]\") \n",
    "sample_dir = \"test_samples\" #\"Directory name to save the image samples [samples]\")\n",
    "result_dir = \"test_result\" #\"Directory name to results of gene imputation [result]\")\n",
    "train = True #\"True for training, False for testing [False]\")\n",
    "g_iter = 2 #\"# Generator Iterations [2]\")\n",
    "bn = False #\"True for batch Norm [False]\")\n",
    "lam = 1.0 #\"Lambda for regularization\")\n",
    "sampler = \"normal\" #\"The sampling distribution of z [uniform, normal, mix_gauss]\")\n",
    "model = \"dra\" #\"Model to train [aae, van_ae] [aae]\")\n",
    "X_dim = hvg #\"Input dimension\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Lu41SpbIteg2",
    "outputId": "d949cf3a-ca39-4fc6-93d0-479597af064f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 19:58:59.610448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5367 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:d8:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (933704, 2000)\n",
      "Shape of data is:  (933704, 2000)\n",
      "here [61 61 61 ... 20 90 40]\n",
      "Data set to work on:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(933704, 2000)\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "(933704, 2)\n",
      "encoder input shape  Tensor(\"concat:0\", shape=(?, 2002), dtype=float32)\n",
      "WARNING:tensorflow:From /home/ajita/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ajita/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "decoder input shape  Tensor(\"concat_2:0\", shape=(?, 12), dtype=float32)\n",
      "KL gaussian z Tensor(\"mul_10:0\", shape=(?,), dtype=float32)\n",
      "KL gaussian l Tensor(\"mul_9:0\", shape=(?,), dtype=float32)\n",
      "WARNING:tensorflow:From /home/ajita/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Cluster DRA on DataSet hum_mou ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 19:59:22.701499: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begins...\n",
      "Epoch : [0] ,  a_loss = 253.5861, d_loss: inf ,  g_loss: inf,  db_loss: 0.3827\n",
      "latent_matrix shape (933704, 10)\n",
      "(933704,)\n",
      "Epoch : [1] ,  a_loss = 233.6028, d_loss: inf ,  g_loss: inf,  db_loss: 1.4273\n",
      "Epoch : [2] ,  a_loss = 196.4098, d_loss: inf ,  g_loss: inf,  db_loss: 1.5261\n",
      "Epoch : [3] ,  a_loss = 147.6511, d_loss: inf ,  g_loss: inf,  db_loss: 0.1229\n",
      "Epoch : [4] ,  a_loss = 269.5229, d_loss: inf ,  g_loss: inf,  db_loss: 0.1099\n",
      "Epoch : [5] ,  a_loss = 159.6469, d_loss: inf ,  g_loss: inf,  db_loss: 0.3405\n",
      "Epoch : [6] ,  a_loss = 210.1671, d_loss: inf ,  g_loss: inf,  db_loss: 0.4075\n",
      "Epoch : [7] ,  a_loss = 237.1931, d_loss: inf ,  g_loss: inf,  db_loss: 0.8971\n",
      "Epoch : [8] ,  a_loss = 307.0327, d_loss: inf ,  g_loss: inf,  db_loss: 0.2021\n",
      "Epoch : [9] ,  a_loss = 177.6340, d_loss: inf ,  g_loss: inf,  db_loss: 0.0961\n",
      "Epoch : [10] ,  a_loss = 247.2111, d_loss: inf ,  g_loss: inf,  db_loss: 0.7154\n",
      "Epoch : [11] ,  a_loss = 123.0648, d_loss: inf ,  g_loss: inf,  db_loss: 0.7168\n",
      "Epoch : [12] ,  a_loss = 158.6539, d_loss: inf ,  g_loss: inf,  db_loss: 0.4199\n",
      "Epoch : [13] ,  a_loss = 108.1709, d_loss: inf ,  g_loss: inf,  db_loss: 0.4098\n",
      "Epoch : [14] ,  a_loss = 142.4307, d_loss: inf ,  g_loss: inf,  db_loss: 0.3621\n",
      "Epoch : [15] ,  a_loss = 177.9951, d_loss: inf ,  g_loss: inf,  db_loss: 0.1496\n",
      "Epoch : [16] ,  a_loss = 154.3259, d_loss: inf ,  g_loss: inf,  db_loss: 0.3638\n",
      "Epoch : [17] ,  a_loss = 318.4128, d_loss: inf ,  g_loss: inf,  db_loss: 0.4269\n",
      "Epoch : [18] ,  a_loss = 186.1257, d_loss: inf ,  g_loss: inf,  db_loss: 0.1757\n",
      "Epoch : [19] ,  a_loss = 157.3040, d_loss: inf ,  g_loss: inf,  db_loss: 0.3839\n",
      "Epoch : [20] ,  a_loss = 149.8133, d_loss: inf ,  g_loss: inf,  db_loss: 0.5190\n",
      "Epoch : [21] ,  a_loss = 176.6272, d_loss: inf ,  g_loss: inf,  db_loss: 0.2898\n",
      "Epoch : [22] ,  a_loss = 175.3044, d_loss: inf ,  g_loss: inf,  db_loss: 0.3811\n",
      "Epoch : [23] ,  a_loss = 189.8735, d_loss: inf ,  g_loss: inf,  db_loss: 0.3854\n",
      "Epoch : [24] ,  a_loss = 198.3426, d_loss: inf ,  g_loss: inf,  db_loss: 0.0759\n",
      "Epoch : [25] ,  a_loss = 166.8893, d_loss: inf ,  g_loss: inf,  db_loss: 1.2489\n",
      "Epoch : [26] ,  a_loss = 175.7150, d_loss: inf ,  g_loss: inf,  db_loss: 0.5772\n",
      "Epoch : [27] ,  a_loss = 133.6230, d_loss: inf ,  g_loss: inf,  db_loss: 0.4161\n",
      "Epoch : [28] ,  a_loss = 185.7471, d_loss: inf ,  g_loss: inf,  db_loss: 0.7610\n",
      "Epoch : [29] ,  a_loss = 209.6049, d_loss: inf ,  g_loss: inf,  db_loss: 0.5129\n",
      "Epoch : [30] ,  a_loss = 125.9417, d_loss: inf ,  g_loss: inf,  db_loss: 0.1146\n",
      "Epoch : [31] ,  a_loss = 220.1268, d_loss: inf ,  g_loss: inf,  db_loss: 0.5981\n",
      "Epoch : [32] ,  a_loss = 190.0371, d_loss: inf ,  g_loss: inf,  db_loss: 0.0711\n",
      "Epoch : [33] ,  a_loss = 430.0560, d_loss: inf ,  g_loss: inf,  db_loss: 0.2456\n",
      "Epoch : [34] ,  a_loss = 199.9192, d_loss: inf ,  g_loss: inf,  db_loss: 0.3178\n",
      "Epoch : [35] ,  a_loss = 159.7155, d_loss: inf ,  g_loss: inf,  db_loss: 0.7972\n",
      "Epoch : [36] ,  a_loss = 159.9379, d_loss: inf ,  g_loss: inf,  db_loss: 0.4278\n",
      "Epoch : [37] ,  a_loss = 143.6326, d_loss: inf ,  g_loss: inf,  db_loss: 0.4533\n",
      "Epoch : [38] ,  a_loss = 158.9147, d_loss: inf ,  g_loss: inf,  db_loss: 0.1809\n",
      "Epoch : [39] ,  a_loss = 195.0895, d_loss: inf ,  g_loss: inf,  db_loss: 0.3634\n",
      "Epoch : [40] ,  a_loss = 239.5715, d_loss: inf ,  g_loss: inf,  db_loss: 0.4616\n",
      "Epoch : [41] ,  a_loss = 201.2866, d_loss: inf ,  g_loss: inf,  db_loss: 1.0643\n",
      "Epoch : [42] ,  a_loss = 138.5221, d_loss: inf ,  g_loss: inf,  db_loss: 0.3072\n",
      "Epoch : [43] ,  a_loss = 172.3957, d_loss: inf ,  g_loss: inf,  db_loss: 2.1715\n",
      "Epoch : [44] ,  a_loss = 209.1958, d_loss: inf ,  g_loss: inf,  db_loss: 0.5395\n",
      "Epoch : [45] ,  a_loss = 139.4077, d_loss: inf ,  g_loss: inf,  db_loss: 0.3271\n",
      "Epoch : [46] ,  a_loss = 139.2541, d_loss: inf ,  g_loss: inf,  db_loss: 0.1331\n",
      "Epoch : [47] ,  a_loss = 220.2702, d_loss: inf ,  g_loss: inf,  db_loss: 1.0550\n",
      "Epoch : [48] ,  a_loss = 188.7774, d_loss: inf ,  g_loss: inf,  db_loss: 0.1589\n",
      "Epoch : [49] ,  a_loss = 152.7843, d_loss: inf ,  g_loss: inf,  db_loss: 0.3157\n",
      "Epoch : [50] ,  a_loss = 192.8935, d_loss: inf ,  g_loss: inf,  db_loss: 0.2415\n",
      "latent_matrix shape (933704, 10)\n",
      "(933704,)\n",
      "Epoch : [51] ,  a_loss = 282.1517, d_loss: inf ,  g_loss: inf,  db_loss: 0.6873\n",
      "Epoch : [52] ,  a_loss = 176.9671, d_loss: inf ,  g_loss: inf,  db_loss: 0.1930\n",
      "Epoch : [53] ,  a_loss = 136.1101, d_loss: inf ,  g_loss: inf,  db_loss: 0.3703\n",
      "Epoch : [54] ,  a_loss = 195.4138, d_loss: inf ,  g_loss: inf,  db_loss: 1.0136\n",
      "Epoch : [55] ,  a_loss = 193.2713, d_loss: inf ,  g_loss: inf,  db_loss: 1.0669\n",
      "Epoch : [56] ,  a_loss = 201.6753, d_loss: inf ,  g_loss: inf,  db_loss: 0.1090\n",
      "Epoch : [57] ,  a_loss = 231.8453, d_loss: inf ,  g_loss: inf,  db_loss: 0.6647\n",
      "Epoch : [58] ,  a_loss = 201.8555, d_loss: inf ,  g_loss: inf,  db_loss: 0.3160\n",
      "Epoch : [59] ,  a_loss = 181.0329, d_loss: inf ,  g_loss: inf,  db_loss: 1.0625\n",
      "Epoch : [60] ,  a_loss = 176.6736, d_loss: inf ,  g_loss: inf,  db_loss: 0.3153\n",
      "Epoch : [61] ,  a_loss = 139.4713, d_loss: inf ,  g_loss: inf,  db_loss: 0.1881\n",
      "Epoch : [62] ,  a_loss = 153.9191, d_loss: inf ,  g_loss: inf,  db_loss: 0.5455\n",
      "Epoch : [63] ,  a_loss = 236.1032, d_loss: inf ,  g_loss: inf,  db_loss: 1.1553\n",
      "Epoch : [64] ,  a_loss = 125.0340, d_loss: inf ,  g_loss: inf,  db_loss: 0.3619\n",
      "Epoch : [65] ,  a_loss = 191.5979, d_loss: inf ,  g_loss: inf,  db_loss: 0.3598\n",
      "Epoch : [66] ,  a_loss = 184.4637, d_loss: inf ,  g_loss: inf,  db_loss: 0.2081\n",
      "Epoch : [67] ,  a_loss = 240.9316, d_loss: inf ,  g_loss: inf,  db_loss: 0.4269\n",
      "Epoch : [68] ,  a_loss = 112.8048, d_loss: inf ,  g_loss: inf,  db_loss: 1.1452\n",
      "Epoch : [69] ,  a_loss = 206.5500, d_loss: inf ,  g_loss: inf,  db_loss: 0.6612\n",
      "Epoch : [70] ,  a_loss = 170.1532, d_loss: inf ,  g_loss: inf,  db_loss: 0.5671\n",
      "Epoch : [71] ,  a_loss = 137.6507, d_loss: inf ,  g_loss: inf,  db_loss: 1.1636\n",
      "Epoch : [72] ,  a_loss = 193.8418, d_loss: inf ,  g_loss: inf,  db_loss: 0.1161\n",
      "Epoch : [73] ,  a_loss = 267.5426, d_loss: inf ,  g_loss: inf,  db_loss: 0.0598\n",
      "Epoch : [74] ,  a_loss = 169.4793, d_loss: inf ,  g_loss: inf,  db_loss: 0.9422\n",
      "Epoch : [75] ,  a_loss = 157.3884, d_loss: inf ,  g_loss: inf,  db_loss: 0.0887\n",
      "Epoch : [76] ,  a_loss = 145.8828, d_loss: inf ,  g_loss: inf,  db_loss: 0.3330\n",
      "Epoch : [77] ,  a_loss = 153.5853, d_loss: inf ,  g_loss: inf,  db_loss: 0.1995\n",
      "Epoch : [78] ,  a_loss = 171.2071, d_loss: inf ,  g_loss: inf,  db_loss: 0.2815\n",
      "Epoch : [79] ,  a_loss = 131.7480, d_loss: inf ,  g_loss: inf,  db_loss: 0.4821\n",
      "Epoch : [80] ,  a_loss = 195.1788, d_loss: inf ,  g_loss: inf,  db_loss: 1.3806\n",
      "Epoch : [81] ,  a_loss = 122.9686, d_loss: inf ,  g_loss: inf,  db_loss: 0.3462\n",
      "Epoch : [82] ,  a_loss = 202.0806, d_loss: inf ,  g_loss: inf,  db_loss: 0.6606\n",
      "Epoch : [83] ,  a_loss = 186.4920, d_loss: inf ,  g_loss: inf,  db_loss: 0.2856\n",
      "Epoch : [84] ,  a_loss = 180.3058, d_loss: inf ,  g_loss: inf,  db_loss: 0.6899\n",
      "Epoch : [85] ,  a_loss = 130.5768, d_loss: inf ,  g_loss: inf,  db_loss: 0.3900\n",
      "Epoch : [86] ,  a_loss = 120.8309, d_loss: inf ,  g_loss: inf,  db_loss: 0.1436\n",
      "Epoch : [87] ,  a_loss = 106.0749, d_loss: inf ,  g_loss: inf,  db_loss: 0.2285\n",
      "Epoch : [88] ,  a_loss = 146.2255, d_loss: inf ,  g_loss: inf,  db_loss: 0.2430\n",
      "Epoch : [89] ,  a_loss = 175.7982, d_loss: inf ,  g_loss: inf,  db_loss: 0.2218\n",
      "Epoch : [90] ,  a_loss = 243.8736, d_loss: inf ,  g_loss: inf,  db_loss: 1.6416\n",
      "Epoch : [91] ,  a_loss = 166.4659, d_loss: inf ,  g_loss: inf,  db_loss: 0.1797\n",
      "Epoch : [92] ,  a_loss = 221.8611, d_loss: inf ,  g_loss: inf,  db_loss: 0.2055\n",
      "Epoch : [93] ,  a_loss = 113.0378, d_loss: inf ,  g_loss: inf,  db_loss: 0.7178\n",
      "Epoch : [94] ,  a_loss = 157.0681, d_loss: inf ,  g_loss: inf,  db_loss: 0.4018\n",
      "Epoch : [95] ,  a_loss = 199.7819, d_loss: inf ,  g_loss: inf,  db_loss: 1.7243\n",
      "Epoch : [96] ,  a_loss = 130.9958, d_loss: inf ,  g_loss: inf,  db_loss: 0.0684\n",
      "Epoch : [97] ,  a_loss = 157.9637, d_loss: inf ,  g_loss: inf,  db_loss: 0.4265\n",
      "Epoch : [98] ,  a_loss = 120.7181, d_loss: inf ,  g_loss: inf,  db_loss: 0.2181\n",
      "Epoch : [99] ,  a_loss = 206.5601, d_loss: inf ,  g_loss: inf,  db_loss: 1.3918\n",
      "Epoch : [100] ,  a_loss = 286.1945, d_loss: inf ,  g_loss: inf,  db_loss: 0.5936\n",
      "latent_matrix shape (933704, 10)\n",
      "(933704,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [101] ,  a_loss = 199.6007, d_loss: inf ,  g_loss: inf,  db_loss: 0.1846\n",
      "Epoch : [102] ,  a_loss = 124.8199, d_loss: inf ,  g_loss: inf,  db_loss: 0.2700\n",
      "Epoch : [103] ,  a_loss = 147.8670, d_loss: inf ,  g_loss: inf,  db_loss: 0.3125\n",
      "Epoch : [104] ,  a_loss = 172.2777, d_loss: inf ,  g_loss: inf,  db_loss: 0.1362\n",
      "Epoch : [105] ,  a_loss = 135.4569, d_loss: inf ,  g_loss: inf,  db_loss: 0.2820\n",
      "Epoch : [106] ,  a_loss = 150.3567, d_loss: inf ,  g_loss: inf,  db_loss: 0.3847\n",
      "Epoch : [107] ,  a_loss = 181.9352, d_loss: inf ,  g_loss: inf,  db_loss: 0.4343\n",
      "Epoch : [108] ,  a_loss = 90.9168, d_loss: inf ,  g_loss: inf,  db_loss: 0.3801\n",
      "Epoch : [109] ,  a_loss = 140.0459, d_loss: inf ,  g_loss: inf,  db_loss: 0.3600\n",
      "Epoch : [110] ,  a_loss = 154.8193, d_loss: inf ,  g_loss: inf,  db_loss: 0.7461\n",
      "Epoch : [111] ,  a_loss = 267.6531, d_loss: inf ,  g_loss: inf,  db_loss: 0.7775\n",
      "Epoch : [112] ,  a_loss = 304.6313, d_loss: inf ,  g_loss: inf,  db_loss: 0.3525\n",
      "Epoch : [113] ,  a_loss = 170.2695, d_loss: inf ,  g_loss: inf,  db_loss: 0.4255\n",
      "Epoch : [114] ,  a_loss = 161.6996, d_loss: inf ,  g_loss: inf,  db_loss: 0.2708\n",
      "Epoch : [115] ,  a_loss = 113.7608, d_loss: inf ,  g_loss: inf,  db_loss: 0.2762\n",
      "Epoch : [116] ,  a_loss = 175.0727, d_loss: inf ,  g_loss: inf,  db_loss: 0.6519\n",
      "Epoch : [117] ,  a_loss = 178.5364, d_loss: inf ,  g_loss: inf,  db_loss: 1.0177\n",
      "Epoch : [118] ,  a_loss = 134.9492, d_loss: inf ,  g_loss: inf,  db_loss: 0.3932\n",
      "Epoch : [119] ,  a_loss = 116.7626, d_loss: inf ,  g_loss: inf,  db_loss: 0.4544\n",
      "Epoch : [120] ,  a_loss = 179.3815, d_loss: inf ,  g_loss: inf,  db_loss: 1.2719\n",
      "Epoch : [121] ,  a_loss = 102.1552, d_loss: inf ,  g_loss: inf,  db_loss: 0.4191\n",
      "Epoch : [122] ,  a_loss = 193.0615, d_loss: inf ,  g_loss: inf,  db_loss: 0.6581\n",
      "Epoch : [123] ,  a_loss = 148.8945, d_loss: inf ,  g_loss: inf,  db_loss: 0.1928\n",
      "Epoch : [124] ,  a_loss = 72.4332, d_loss: inf ,  g_loss: inf,  db_loss: 0.1380\n",
      "Epoch : [125] ,  a_loss = 147.4788, d_loss: inf ,  g_loss: inf,  db_loss: 0.0590\n",
      "Epoch : [126] ,  a_loss = 142.4985, d_loss: inf ,  g_loss: inf,  db_loss: 0.2056\n",
      "Epoch : [127] ,  a_loss = 108.1876, d_loss: inf ,  g_loss: inf,  db_loss: 0.3999\n",
      "Epoch : [128] ,  a_loss = 94.7456, d_loss: inf ,  g_loss: inf,  db_loss: 0.1448\n",
      "Epoch : [129] ,  a_loss = 140.5724, d_loss: inf ,  g_loss: inf,  db_loss: 1.4992\n",
      "Epoch : [130] ,  a_loss = 163.6493, d_loss: inf ,  g_loss: inf,  db_loss: 1.7406\n",
      "Epoch : [131] ,  a_loss = 134.4657, d_loss: inf ,  g_loss: inf,  db_loss: 0.1848\n",
      "Epoch : [132] ,  a_loss = 179.3933, d_loss: inf ,  g_loss: inf,  db_loss: 1.1685\n",
      "Epoch : [133] ,  a_loss = 173.1134, d_loss: inf ,  g_loss: inf,  db_loss: 0.4198\n",
      "Epoch : [134] ,  a_loss = 242.4388, d_loss: inf ,  g_loss: inf,  db_loss: 0.5562\n",
      "Epoch : [135] ,  a_loss = 150.9312, d_loss: inf ,  g_loss: inf,  db_loss: 0.5130\n",
      "Epoch : [136] ,  a_loss = 194.0521, d_loss: inf ,  g_loss: inf,  db_loss: 0.6391\n",
      "Epoch : [137] ,  a_loss = 216.6439, d_loss: inf ,  g_loss: inf,  db_loss: 0.2048\n",
      "Epoch : [138] ,  a_loss = 122.2035, d_loss: inf ,  g_loss: inf,  db_loss: 0.6951\n",
      "Epoch : [139] ,  a_loss = 118.6479, d_loss: inf ,  g_loss: inf,  db_loss: 0.5744\n",
      "Epoch : [140] ,  a_loss = 172.8212, d_loss: inf ,  g_loss: inf,  db_loss: 1.1220\n",
      "Epoch : [141] ,  a_loss = 130.4124, d_loss: inf ,  g_loss: inf,  db_loss: 0.1610\n",
      "Epoch : [142] ,  a_loss = 193.1429, d_loss: inf ,  g_loss: inf,  db_loss: 0.3049\n",
      "Epoch : [143] ,  a_loss = 89.5816, d_loss: inf ,  g_loss: inf,  db_loss: 0.3519\n",
      "Epoch : [144] ,  a_loss = 134.9542, d_loss: inf ,  g_loss: inf,  db_loss: 0.1402\n",
      "Epoch : [145] ,  a_loss = 203.8353, d_loss: inf ,  g_loss: inf,  db_loss: 0.4052\n",
      "Epoch : [146] ,  a_loss = 137.4946, d_loss: inf ,  g_loss: inf,  db_loss: 1.6306\n",
      "Epoch : [147] ,  a_loss = 135.9129, d_loss: inf ,  g_loss: inf,  db_loss: 0.3279\n",
      "Epoch : [148] ,  a_loss = 121.8130, d_loss: inf ,  g_loss: inf,  db_loss: 0.2780\n",
      "Epoch : [149] ,  a_loss = 162.2369, d_loss: inf ,  g_loss: inf,  db_loss: 0.5561\n",
      "Epoch : [150] ,  a_loss = 168.9583, d_loss: inf ,  g_loss: inf,  db_loss: 0.1762\n",
      "latent_matrix shape (933704, 10)\n",
      "(933704,)\n",
      "Epoch : [151] ,  a_loss = 203.0313, d_loss: inf ,  g_loss: inf,  db_loss: 0.6732\n",
      "Epoch : [152] ,  a_loss = 182.0000, d_loss: inf ,  g_loss: inf,  db_loss: 0.0294\n",
      "Epoch : [153] ,  a_loss = 115.7338, d_loss: inf ,  g_loss: inf,  db_loss: 0.3160\n",
      "Epoch : [154] ,  a_loss = 135.2048, d_loss: inf ,  g_loss: inf,  db_loss: 0.3821\n",
      "Epoch : [155] ,  a_loss = 221.5769, d_loss: inf ,  g_loss: inf,  db_loss: 1.2912\n",
      "Epoch : [156] ,  a_loss = 110.4725, d_loss: inf ,  g_loss: inf,  db_loss: 0.5647\n",
      "Epoch : [157] ,  a_loss = 199.0579, d_loss: inf ,  g_loss: inf,  db_loss: 0.2542\n",
      "Epoch : [158] ,  a_loss = 103.9292, d_loss: inf ,  g_loss: inf,  db_loss: 0.2940\n",
      "Epoch : [159] ,  a_loss = 327.0184, d_loss: inf ,  g_loss: inf,  db_loss: 0.7998\n",
      "Epoch : [160] ,  a_loss = 207.0823, d_loss: inf ,  g_loss: inf,  db_loss: 0.6255\n",
      "Epoch : [161] ,  a_loss = 157.1661, d_loss: inf ,  g_loss: inf,  db_loss: 0.3593\n",
      "Epoch : [162] ,  a_loss = 241.3528, d_loss: inf ,  g_loss: inf,  db_loss: 0.7371\n",
      "Epoch : [163] ,  a_loss = 173.1450, d_loss: inf ,  g_loss: inf,  db_loss: 1.3807\n",
      "Epoch : [164] ,  a_loss = 150.2453, d_loss: inf ,  g_loss: inf,  db_loss: 0.1554\n",
      "Epoch : [165] ,  a_loss = 91.9671, d_loss: inf ,  g_loss: inf,  db_loss: 0.6054\n",
      "Epoch : [166] ,  a_loss = 139.0350, d_loss: inf ,  g_loss: inf,  db_loss: 0.2507\n",
      "Epoch : [167] ,  a_loss = 155.1646, d_loss: inf ,  g_loss: inf,  db_loss: 0.2976\n",
      "Epoch : [168] ,  a_loss = 241.2114, d_loss: inf ,  g_loss: inf,  db_loss: 0.2488\n",
      "Epoch : [169] ,  a_loss = 176.7358, d_loss: inf ,  g_loss: inf,  db_loss: 1.0532\n",
      "Epoch : [170] ,  a_loss = 173.6416, d_loss: inf ,  g_loss: inf,  db_loss: 1.4919\n",
      "Epoch : [171] ,  a_loss = 135.1999, d_loss: inf ,  g_loss: inf,  db_loss: 0.3287\n",
      "Epoch : [172] ,  a_loss = 205.8219, d_loss: inf ,  g_loss: inf,  db_loss: 0.6899\n",
      "Epoch : [173] ,  a_loss = 114.5764, d_loss: inf ,  g_loss: inf,  db_loss: 0.4557\n",
      "Epoch : [174] ,  a_loss = 199.7418, d_loss: inf ,  g_loss: inf,  db_loss: 1.5322\n",
      "Epoch : [175] ,  a_loss = 202.7365, d_loss: inf ,  g_loss: inf,  db_loss: 0.7056\n",
      "Epoch : [176] ,  a_loss = 294.2928, d_loss: inf ,  g_loss: inf,  db_loss: 0.5890\n",
      "Epoch : [177] ,  a_loss = 128.9205, d_loss: inf ,  g_loss: inf,  db_loss: 0.3610\n",
      "Epoch : [178] ,  a_loss = 133.1432, d_loss: inf ,  g_loss: inf,  db_loss: 0.5314\n",
      "Epoch : [179] ,  a_loss = 170.2620, d_loss: inf ,  g_loss: inf,  db_loss: 0.2746\n",
      "Epoch : [180] ,  a_loss = 353.9964, d_loss: inf ,  g_loss: inf,  db_loss: 0.6539\n",
      "Epoch : [181] ,  a_loss = 98.8011, d_loss: inf ,  g_loss: inf,  db_loss: 0.1696\n",
      "Epoch : [182] ,  a_loss = 170.5909, d_loss: inf ,  g_loss: inf,  db_loss: 1.0563\n",
      "Epoch : [183] ,  a_loss = 150.3628, d_loss: inf ,  g_loss: inf,  db_loss: 0.1807\n",
      "Epoch : [184] ,  a_loss = 247.6174, d_loss: inf ,  g_loss: inf,  db_loss: 0.3473\n",
      "Epoch : [185] ,  a_loss = 332.6205, d_loss: inf ,  g_loss: inf,  db_loss: 0.0977\n",
      "Epoch : [186] ,  a_loss = 126.1112, d_loss: inf ,  g_loss: inf,  db_loss: 0.0593\n",
      "Epoch : [187] ,  a_loss = 101.7287, d_loss: inf ,  g_loss: inf,  db_loss: 0.1787\n",
      "Epoch : [188] ,  a_loss = 95.7088, d_loss: inf ,  g_loss: inf,  db_loss: 0.5078\n",
      "Epoch : [189] ,  a_loss = 113.8329, d_loss: inf ,  g_loss: inf,  db_loss: 0.7373\n",
      "Epoch : [190] ,  a_loss = 127.0799, d_loss: inf ,  g_loss: inf,  db_loss: 0.2779\n",
      "Epoch : [191] ,  a_loss = 143.7379, d_loss: inf ,  g_loss: inf,  db_loss: 0.4747\n",
      "Epoch : [192] ,  a_loss = 142.9747, d_loss: inf ,  g_loss: inf,  db_loss: 0.0911\n",
      "Epoch : [193] ,  a_loss = 234.5813, d_loss: inf ,  g_loss: inf,  db_loss: 0.0639\n",
      "Epoch : [194] ,  a_loss = 197.4232, d_loss: inf ,  g_loss: inf,  db_loss: 0.0794\n",
      "Epoch : [195] ,  a_loss = 91.5428, d_loss: inf ,  g_loss: inf,  db_loss: 0.3433\n",
      "Epoch : [196] ,  a_loss = 113.4478, d_loss: inf ,  g_loss: inf,  db_loss: 0.3317\n",
      "Epoch : [197] ,  a_loss = 119.4858, d_loss: inf ,  g_loss: inf,  db_loss: 0.6096\n",
      "Epoch : [198] ,  a_loss = 83.2487, d_loss: inf ,  g_loss: inf,  db_loss: 0.4905\n",
      "Epoch : [199] ,  a_loss = 132.9826, d_loss: inf ,  g_loss: inf,  db_loss: 0.4849\n",
      "Epoch : [200] ,  a_loss = 168.6270, d_loss: inf ,  g_loss: inf,  db_loss: 0.2229\n",
      "latent_matrix shape (933704, 10)\n",
      "(933704,)\n",
      "Epoch : [201] ,  a_loss = 122.4002, d_loss: inf ,  g_loss: inf,  db_loss: 0.3303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [202] ,  a_loss = 171.3633, d_loss: inf ,  g_loss: inf,  db_loss: 1.2301\n",
      "Epoch : [203] ,  a_loss = 240.1378, d_loss: inf ,  g_loss: inf,  db_loss: 0.6753\n",
      "Epoch : [204] ,  a_loss = 159.8762, d_loss: inf ,  g_loss: inf,  db_loss: 0.1907\n",
      "Epoch : [205] ,  a_loss = 116.6159, d_loss: inf ,  g_loss: inf,  db_loss: 0.3641\n",
      "Epoch : [206] ,  a_loss = 213.1497, d_loss: inf ,  g_loss: inf,  db_loss: 0.3883\n",
      "Epoch : [207] ,  a_loss = 156.6532, d_loss: inf ,  g_loss: inf,  db_loss: 0.0409\n",
      "Epoch : [208] ,  a_loss = 133.5627, d_loss: inf ,  g_loss: inf,  db_loss: 0.0419\n",
      "Epoch : [209] ,  a_loss = 225.8715, d_loss: inf ,  g_loss: inf,  db_loss: 0.7837\n",
      "Epoch : [210] ,  a_loss = 126.5704, d_loss: inf ,  g_loss: inf,  db_loss: 0.3836\n",
      "Epoch : [211] ,  a_loss = 132.8145, d_loss: inf ,  g_loss: inf,  db_loss: 0.2498\n",
      "Epoch : [212] ,  a_loss = 326.7375, d_loss: inf ,  g_loss: inf,  db_loss: 2.0687\n",
      "Epoch : [213] ,  a_loss = 112.1445, d_loss: inf ,  g_loss: inf,  db_loss: 0.2924\n",
      "Epoch : [214] ,  a_loss = 207.0571, d_loss: inf ,  g_loss: inf,  db_loss: 1.5342\n",
      "Epoch : [215] ,  a_loss = 170.4559, d_loss: inf ,  g_loss: inf,  db_loss: 0.9759\n",
      "Epoch : [216] ,  a_loss = 191.1516, d_loss: inf ,  g_loss: inf,  db_loss: 0.9693\n",
      "Epoch : [217] ,  a_loss = 139.5204, d_loss: inf ,  g_loss: inf,  db_loss: 0.1651\n",
      "Epoch : [218] ,  a_loss = 180.3264, d_loss: inf ,  g_loss: inf,  db_loss: 1.0655\n",
      "Epoch : [219] ,  a_loss = 150.0437, d_loss: inf ,  g_loss: inf,  db_loss: 0.1792\n",
      "Epoch : [220] ,  a_loss = 149.8258, d_loss: inf ,  g_loss: inf,  db_loss: 0.4884\n",
      "Epoch : [221] ,  a_loss = 161.9516, d_loss: inf ,  g_loss: inf,  db_loss: 0.2718\n",
      "Epoch : [222] ,  a_loss = 270.1690, d_loss: inf ,  g_loss: inf,  db_loss: 0.5401\n",
      "Epoch : [223] ,  a_loss = 218.2850, d_loss: inf ,  g_loss: inf,  db_loss: 0.4417\n",
      "Epoch : [224] ,  a_loss = 271.2952, d_loss: inf ,  g_loss: inf,  db_loss: 0.7229\n",
      "Epoch : [225] ,  a_loss = 88.6505, d_loss: inf ,  g_loss: inf,  db_loss: 0.1767\n",
      "Epoch : [226] ,  a_loss = 129.0832, d_loss: inf ,  g_loss: inf,  db_loss: 0.2838\n",
      "Epoch : [227] ,  a_loss = 182.2065, d_loss: inf ,  g_loss: inf,  db_loss: 0.1823\n",
      "Epoch : [228] ,  a_loss = 173.8716, d_loss: inf ,  g_loss: inf,  db_loss: 0.5252\n",
      "Epoch : [229] ,  a_loss = 148.9085, d_loss: inf ,  g_loss: inf,  db_loss: 0.6122\n",
      "Epoch : [230] ,  a_loss = 139.1254, d_loss: inf ,  g_loss: inf,  db_loss: 0.4302\n",
      "Epoch : [231] ,  a_loss = 147.2592, d_loss: inf ,  g_loss: inf,  db_loss: 1.2016\n",
      "Epoch : [232] ,  a_loss = 118.4065, d_loss: inf ,  g_loss: inf,  db_loss: 0.5555\n",
      "Epoch : [233] ,  a_loss = 118.2742, d_loss: inf ,  g_loss: inf,  db_loss: 0.3166\n",
      "Epoch : [234] ,  a_loss = 156.4044, d_loss: inf ,  g_loss: inf,  db_loss: 0.0957\n",
      "Epoch : [235] ,  a_loss = 189.5081, d_loss: inf ,  g_loss: inf,  db_loss: 0.3406\n",
      "Epoch : [236] ,  a_loss = 134.5609, d_loss: inf ,  g_loss: inf,  db_loss: 0.3761\n",
      "Epoch : [237] ,  a_loss = 122.3565, d_loss: inf ,  g_loss: inf,  db_loss: 0.2249\n",
      "Epoch : [238] ,  a_loss = 118.1112, d_loss: inf ,  g_loss: inf,  db_loss: 1.9796\n",
      "Epoch : [239] ,  a_loss = 219.9271, d_loss: inf ,  g_loss: inf,  db_loss: 0.1655\n",
      "Epoch : [240] ,  a_loss = 142.9664, d_loss: inf ,  g_loss: inf,  db_loss: 1.7851\n",
      "Epoch : [241] ,  a_loss = 214.3843, d_loss: inf ,  g_loss: inf,  db_loss: 1.2640\n",
      "Epoch : [242] ,  a_loss = 137.4074, d_loss: inf ,  g_loss: inf,  db_loss: 0.1078\n",
      "Epoch : [243] ,  a_loss = 197.0297, d_loss: inf ,  g_loss: inf,  db_loss: 0.6270\n",
      "Epoch : [244] ,  a_loss = 164.9645, d_loss: inf ,  g_loss: inf,  db_loss: 0.9873\n",
      "Epoch : [245] ,  a_loss = 222.0623, d_loss: inf ,  g_loss: inf,  db_loss: 0.8737\n",
      "Epoch : [246] ,  a_loss = 153.5231, d_loss: inf ,  g_loss: inf,  db_loss: 0.3105\n",
      "Epoch : [247] ,  a_loss = 120.3310, d_loss: inf ,  g_loss: inf,  db_loss: 0.4283\n",
      "Epoch : [248] ,  a_loss = 132.3876, d_loss: inf ,  g_loss: inf,  db_loss: 0.1491\n",
      "Epoch : [249] ,  a_loss = 166.2169, d_loss: inf ,  g_loss: inf,  db_loss: 0.4397\n",
      "Epoch : [250] ,  a_loss = 147.1070, d_loss: inf ,  g_loss: inf,  db_loss: 1.2244\n",
      "latent_matrix shape (933704, 10)\n",
      "(933704,)\n",
      "Epoch : [251] ,  a_loss = 158.3401, d_loss: inf ,  g_loss: inf,  db_loss: 0.1598\n",
      "Epoch : [252] ,  a_loss = 126.2374, d_loss: inf ,  g_loss: inf,  db_loss: 0.2602\n",
      "Epoch : [253] ,  a_loss = 133.8918, d_loss: inf ,  g_loss: inf,  db_loss: 0.0448\n",
      "Epoch : [254] ,  a_loss = 106.9072, d_loss: inf ,  g_loss: inf,  db_loss: 0.1667\n",
      "Epoch : [255] ,  a_loss = 351.4236, d_loss: inf ,  g_loss: inf,  db_loss: 1.1322\n",
      "Epoch : [256] ,  a_loss = 106.8495, d_loss: inf ,  g_loss: inf,  db_loss: 0.1690\n",
      "Epoch : [257] ,  a_loss = 120.2300, d_loss: inf ,  g_loss: inf,  db_loss: 0.3000\n",
      "Epoch : [258] ,  a_loss = 214.9067, d_loss: inf ,  g_loss: inf,  db_loss: 0.5976\n",
      "Epoch : [259] ,  a_loss = 131.0117, d_loss: inf ,  g_loss: inf,  db_loss: 0.2377\n",
      "Epoch : [260] ,  a_loss = 142.8023, d_loss: inf ,  g_loss: inf,  db_loss: 0.1273\n",
      "Epoch : [261] ,  a_loss = 132.1991, d_loss: inf ,  g_loss: inf,  db_loss: 0.2992\n",
      "Epoch : [262] ,  a_loss = 248.4458, d_loss: inf ,  g_loss: inf,  db_loss: 0.5997\n",
      "Epoch : [263] ,  a_loss = 165.9401, d_loss: inf ,  g_loss: inf,  db_loss: 0.3794\n",
      "Epoch : [264] ,  a_loss = 166.2077, d_loss: inf ,  g_loss: inf,  db_loss: 0.4952\n",
      "Epoch : [265] ,  a_loss = 193.3678, d_loss: inf ,  g_loss: inf,  db_loss: 1.0709\n",
      "Epoch : [266] ,  a_loss = 100.4379, d_loss: inf ,  g_loss: inf,  db_loss: 0.3665\n",
      "Epoch : [267] ,  a_loss = 180.9871, d_loss: inf ,  g_loss: inf,  db_loss: 0.2911\n",
      "Epoch : [268] ,  a_loss = 142.8699, d_loss: inf ,  g_loss: inf,  db_loss: 0.1745\n",
      "Epoch : [269] ,  a_loss = 194.6324, d_loss: inf ,  g_loss: inf,  db_loss: 0.5461\n",
      "Epoch : [270] ,  a_loss = 240.3238, d_loss: inf ,  g_loss: inf,  db_loss: 0.1642\n",
      "Epoch : [271] ,  a_loss = 191.9615, d_loss: inf ,  g_loss: inf,  db_loss: 0.4736\n",
      "Epoch : [272] ,  a_loss = 147.1121, d_loss: inf ,  g_loss: inf,  db_loss: 0.1879\n",
      "Epoch : [273] ,  a_loss = 188.4283, d_loss: inf ,  g_loss: inf,  db_loss: 0.2666\n",
      "Epoch : [274] ,  a_loss = 216.7294, d_loss: inf ,  g_loss: inf,  db_loss: 0.5554\n",
      "Epoch : [275] ,  a_loss = 124.6774, d_loss: inf ,  g_loss: inf,  db_loss: 0.0840\n",
      "Epoch : [276] ,  a_loss = 189.1402, d_loss: inf ,  g_loss: inf,  db_loss: 0.4563\n",
      "Epoch : [277] ,  a_loss = 185.1327, d_loss: inf ,  g_loss: inf,  db_loss: 0.2269\n",
      "Epoch : [278] ,  a_loss = 244.6915, d_loss: inf ,  g_loss: inf,  db_loss: 0.7235\n",
      "Epoch : [279] ,  a_loss = 255.1895, d_loss: inf ,  g_loss: inf,  db_loss: 0.4509\n",
      "Epoch : [280] ,  a_loss = 166.4628, d_loss: inf ,  g_loss: inf,  db_loss: 0.6828\n",
      "Epoch : [281] ,  a_loss = 101.5655, d_loss: inf ,  g_loss: inf,  db_loss: 0.3536\n",
      "Epoch : [282] ,  a_loss = 168.7486, d_loss: inf ,  g_loss: inf,  db_loss: 0.1431\n",
      "Epoch : [283] ,  a_loss = 164.5277, d_loss: inf ,  g_loss: inf,  db_loss: 0.3772\n",
      "Epoch : [284] ,  a_loss = 186.8426, d_loss: inf ,  g_loss: inf,  db_loss: 1.1675\n",
      "Epoch : [285] ,  a_loss = 228.2227, d_loss: inf ,  g_loss: inf,  db_loss: 0.2248\n",
      "Epoch : [286] ,  a_loss = 120.8623, d_loss: inf ,  g_loss: inf,  db_loss: 0.4561\n",
      "Epoch : [287] ,  a_loss = 165.4249, d_loss: inf ,  g_loss: inf,  db_loss: 0.3293\n",
      "Epoch : [288] ,  a_loss = 143.7288, d_loss: inf ,  g_loss: inf,  db_loss: 0.5562\n",
      "Epoch : [289] ,  a_loss = 195.5538, d_loss: inf ,  g_loss: inf,  db_loss: 1.0695\n",
      "Epoch : [290] ,  a_loss = 202.3659, d_loss: inf ,  g_loss: inf,  db_loss: 0.1955\n",
      "Epoch : [291] ,  a_loss = 223.2376, d_loss: inf ,  g_loss: inf,  db_loss: 0.7282\n",
      "Epoch : [292] ,  a_loss = 129.0682, d_loss: inf ,  g_loss: inf,  db_loss: 0.4770\n",
      "Epoch : [293] ,  a_loss = 168.2544, d_loss: inf ,  g_loss: inf,  db_loss: 0.1922\n",
      "Epoch : [294] ,  a_loss = 143.9084, d_loss: inf ,  g_loss: inf,  db_loss: 0.0721\n",
      "Epoch : [295] ,  a_loss = 129.4198, d_loss: inf ,  g_loss: inf,  db_loss: 0.4896\n",
      "Epoch : [296] ,  a_loss = 157.0479, d_loss: inf ,  g_loss: inf,  db_loss: 0.6905\n",
      "Epoch : [297] ,  a_loss = 126.8153, d_loss: inf ,  g_loss: inf,  db_loss: 0.4212\n",
      "Epoch : [298] ,  a_loss = 255.5234, d_loss: inf ,  g_loss: inf,  db_loss: 0.1643\n",
      "Epoch : [299] ,  a_loss = 170.4647, d_loss: inf ,  g_loss: inf,  db_loss: 0.2230\n",
      "Epoch : [300] ,  a_loss = 157.1220, d_loss: inf ,  g_loss: inf,  db_loss: 0.4688\n",
      "latent_matrix shape (933704, 10)\n",
      "(933704,)\n",
      "Epoch : [301] ,  a_loss = 139.8256, d_loss: inf ,  g_loss: inf,  db_loss: 0.5546\n",
      "Epoch : [302] ,  a_loss = 180.2369, d_loss: inf ,  g_loss: inf,  db_loss: 0.2204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [303] ,  a_loss = 132.8000, d_loss: inf ,  g_loss: inf,  db_loss: 0.1507\n",
      "Epoch : [304] ,  a_loss = 106.2487, d_loss: inf ,  g_loss: inf,  db_loss: 0.2761\n",
      "Epoch : [305] ,  a_loss = 150.3975, d_loss: inf ,  g_loss: inf,  db_loss: 1.3940\n",
      "Epoch : [306] ,  a_loss = 212.0852, d_loss: inf ,  g_loss: inf,  db_loss: 0.2833\n",
      "Epoch : [307] ,  a_loss = 304.0639, d_loss: inf ,  g_loss: inf,  db_loss: 0.1725\n",
      "Epoch : [308] ,  a_loss = 104.5381, d_loss: inf ,  g_loss: inf,  db_loss: 0.5358\n",
      "Epoch : [309] ,  a_loss = 127.0542, d_loss: inf ,  g_loss: inf,  db_loss: 0.5313\n",
      "Epoch : [310] ,  a_loss = 180.8510, d_loss: inf ,  g_loss: inf,  db_loss: 1.0164\n",
      "Epoch : [311] ,  a_loss = 132.9726, d_loss: inf ,  g_loss: inf,  db_loss: 0.8406\n",
      "Epoch : [312] ,  a_loss = 179.2695, d_loss: inf ,  g_loss: inf,  db_loss: 1.6606\n",
      "Epoch : [313] ,  a_loss = 256.9315, d_loss: inf ,  g_loss: inf,  db_loss: 1.3851\n",
      "Epoch : [314] ,  a_loss = 169.4160, d_loss: inf ,  g_loss: inf,  db_loss: 0.3275\n",
      "Epoch : [315] ,  a_loss = 221.5244, d_loss: inf ,  g_loss: inf,  db_loss: 0.4268\n",
      "Epoch : [316] ,  a_loss = 132.7249, d_loss: inf ,  g_loss: inf,  db_loss: 1.0267\n",
      "Epoch : [317] ,  a_loss = 150.6051, d_loss: inf ,  g_loss: inf,  db_loss: 0.3159\n",
      "Epoch : [318] ,  a_loss = 159.3022, d_loss: inf ,  g_loss: inf,  db_loss: 0.7641\n",
      "Epoch : [319] ,  a_loss = 121.7613, d_loss: inf ,  g_loss: inf,  db_loss: 0.5064\n",
      "Epoch : [320] ,  a_loss = 142.6235, d_loss: inf ,  g_loss: inf,  db_loss: 0.7044\n",
      "Epoch : [321] ,  a_loss = 179.4493, d_loss: inf ,  g_loss: inf,  db_loss: 0.7423\n",
      "Epoch : [322] ,  a_loss = 161.8688, d_loss: inf ,  g_loss: inf,  db_loss: 1.1150\n",
      "Epoch : [323] ,  a_loss = 223.9338, d_loss: inf ,  g_loss: inf,  db_loss: 0.9366\n",
      "Epoch : [324] ,  a_loss = 147.4694, d_loss: inf ,  g_loss: inf,  db_loss: 0.3713\n",
      "Epoch : [325] ,  a_loss = 115.6372, d_loss: inf ,  g_loss: inf,  db_loss: 0.3664\n",
      "Epoch : [326] ,  a_loss = 141.0928, d_loss: inf ,  g_loss: inf,  db_loss: 0.0710\n",
      "Epoch : [327] ,  a_loss = 224.9222, d_loss: inf ,  g_loss: inf,  db_loss: 0.0391\n",
      "Epoch : [328] ,  a_loss = 152.5701, d_loss: inf ,  g_loss: inf,  db_loss: 0.6347\n",
      "Epoch : [329] ,  a_loss = 146.0447, d_loss: inf ,  g_loss: inf,  db_loss: 0.2324\n",
      "Epoch : [330] ,  a_loss = 119.8433, d_loss: inf ,  g_loss: inf,  db_loss: 0.1200\n",
      "Epoch : [331] ,  a_loss = 200.8632, d_loss: inf ,  g_loss: inf,  db_loss: 0.3471\n",
      "Epoch : [332] ,  a_loss = 108.4252, d_loss: inf ,  g_loss: inf,  db_loss: 0.3733\n",
      "Epoch : [333] ,  a_loss = 157.7036, d_loss: inf ,  g_loss: inf,  db_loss: 0.6937\n",
      "Epoch : [334] ,  a_loss = 113.2257, d_loss: inf ,  g_loss: inf,  db_loss: 0.3398\n",
      "Epoch : [335] ,  a_loss = 104.2996, d_loss: inf ,  g_loss: inf,  db_loss: 0.2991\n",
      "Epoch : [336] ,  a_loss = 123.1262, d_loss: inf ,  g_loss: inf,  db_loss: 0.1918\n",
      "Epoch : [337] ,  a_loss = 178.8204, d_loss: inf ,  g_loss: inf,  db_loss: 0.2445\n",
      "Epoch : [338] ,  a_loss = 232.4529, d_loss: inf ,  g_loss: inf,  db_loss: 1.0589\n",
      "Epoch : [339] ,  a_loss = 191.1623, d_loss: inf ,  g_loss: inf,  db_loss: 0.7821\n",
      "Epoch : [340] ,  a_loss = 172.2753, d_loss: inf ,  g_loss: inf,  db_loss: 0.2173\n",
      "Epoch : [341] ,  a_loss = 186.2493, d_loss: inf ,  g_loss: inf,  db_loss: 0.1823\n",
      "Epoch : [342] ,  a_loss = 252.5710, d_loss: inf ,  g_loss: inf,  db_loss: 0.5818\n",
      "Epoch : [343] ,  a_loss = 124.4632, d_loss: inf ,  g_loss: inf,  db_loss: 0.6183\n",
      "Epoch : [344] ,  a_loss = 139.3228, d_loss: inf ,  g_loss: inf,  db_loss: 0.1630\n",
      "Epoch : [345] ,  a_loss = 107.1217, d_loss: inf ,  g_loss: inf,  db_loss: 0.1953\n",
      "Epoch : [346] ,  a_loss = 149.6198, d_loss: inf ,  g_loss: inf,  db_loss: 1.2956\n",
      "Epoch : [347] ,  a_loss = 182.1711, d_loss: inf ,  g_loss: inf,  db_loss: 0.5962\n",
      "Epoch : [348] ,  a_loss = 152.2433, d_loss: inf ,  g_loss: inf,  db_loss: 0.3034\n",
      "Epoch : [349] ,  a_loss = 234.1853, d_loss: inf ,  g_loss: inf,  db_loss: 0.2968\n",
      "Epoch : [350] ,  a_loss = 152.4602, d_loss: inf ,  g_loss: inf,  db_loss: 0.1448\n",
      "latent_matrix shape (933704, 10)\n",
      "(933704,)\n",
      "Epoch : [351] ,  a_loss = 221.1631, d_loss: inf ,  g_loss: inf,  db_loss: 0.6480\n",
      "Epoch : [352] ,  a_loss = 174.1074, d_loss: inf ,  g_loss: inf,  db_loss: 0.9861\n",
      "Epoch : [353] ,  a_loss = 129.9801, d_loss: inf ,  g_loss: inf,  db_loss: 0.3390\n",
      "Epoch : [354] ,  a_loss = 164.9462, d_loss: inf ,  g_loss: inf,  db_loss: 0.3599\n",
      "Epoch : [355] ,  a_loss = 132.9599, d_loss: inf ,  g_loss: inf,  db_loss: 0.8663\n",
      "Epoch : [356] ,  a_loss = 157.8443, d_loss: inf ,  g_loss: inf,  db_loss: 0.1819\n",
      "Epoch : [357] ,  a_loss = 103.2044, d_loss: inf ,  g_loss: inf,  db_loss: 0.4554\n",
      "Epoch : [358] ,  a_loss = 152.4662, d_loss: inf ,  g_loss: inf,  db_loss: 1.7640\n",
      "Epoch : [359] ,  a_loss = 155.8999, d_loss: inf ,  g_loss: inf,  db_loss: 0.2421\n",
      "Epoch : [360] ,  a_loss = 181.2616, d_loss: inf ,  g_loss: inf,  db_loss: 0.4236\n",
      "Epoch : [361] ,  a_loss = 116.5267, d_loss: inf ,  g_loss: inf,  db_loss: 0.2645\n",
      "Epoch : [362] ,  a_loss = 168.5410, d_loss: inf ,  g_loss: inf,  db_loss: 0.4033\n",
      "Epoch : [363] ,  a_loss = 137.5871, d_loss: inf ,  g_loss: inf,  db_loss: 0.4077\n",
      "Epoch : [364] ,  a_loss = 123.0370, d_loss: inf ,  g_loss: inf,  db_loss: 0.3951\n",
      "Epoch : [365] ,  a_loss = 118.6910, d_loss: inf ,  g_loss: inf,  db_loss: 0.2628\n",
      "Epoch : [366] ,  a_loss = 179.3796, d_loss: inf ,  g_loss: inf,  db_loss: 0.0856\n",
      "Epoch : [367] ,  a_loss = 178.6044, d_loss: inf ,  g_loss: inf,  db_loss: 0.2718\n",
      "Epoch : [368] ,  a_loss = 138.0781, d_loss: inf ,  g_loss: inf,  db_loss: 0.0901\n",
      "Epoch : [369] ,  a_loss = 159.7818, d_loss: inf ,  g_loss: inf,  db_loss: 0.3449\n",
      "Epoch : [370] ,  a_loss = 176.1782, d_loss: inf ,  g_loss: inf,  db_loss: 0.4331\n",
      "Epoch : [371] ,  a_loss = 152.8623, d_loss: inf ,  g_loss: inf,  db_loss: 0.3432\n",
      "Epoch : [372] ,  a_loss = 187.7415, d_loss: inf ,  g_loss: inf,  db_loss: 0.8881\n",
      "Epoch : [373] ,  a_loss = 149.8600, d_loss: inf ,  g_loss: inf,  db_loss: 0.3184\n",
      "Epoch : [374] ,  a_loss = 107.1085, d_loss: inf ,  g_loss: inf,  db_loss: 0.5133\n",
      "Epoch : [375] ,  a_loss = 154.1268, d_loss: inf ,  g_loss: inf,  db_loss: 0.3489\n",
      "Epoch : [376] ,  a_loss = 153.0959, d_loss: inf ,  g_loss: inf,  db_loss: 1.0406\n",
      "Epoch : [377] ,  a_loss = 240.3098, d_loss: inf ,  g_loss: inf,  db_loss: 0.1258\n",
      "Epoch : [378] ,  a_loss = 126.0868, d_loss: inf ,  g_loss: inf,  db_loss: 0.4325\n",
      "Epoch : [379] ,  a_loss = 184.7815, d_loss: inf ,  g_loss: inf,  db_loss: 0.5058\n",
      "Epoch : [380] ,  a_loss = 155.4983, d_loss: inf ,  g_loss: inf,  db_loss: 1.9387\n",
      "Epoch : [381] ,  a_loss = 152.5448, d_loss: inf ,  g_loss: inf,  db_loss: 0.1619\n",
      "Epoch : [382] ,  a_loss = 113.1396, d_loss: inf ,  g_loss: inf,  db_loss: 0.3836\n",
      "Epoch : [383] ,  a_loss = 143.2493, d_loss: inf ,  g_loss: inf,  db_loss: 0.1057\n",
      "Epoch : [384] ,  a_loss = 93.9292, d_loss: inf ,  g_loss: inf,  db_loss: 0.2169\n",
      "Epoch : [385] ,  a_loss = 125.7536, d_loss: inf ,  g_loss: inf,  db_loss: 0.3073\n",
      "Epoch : [386] ,  a_loss = 160.8592, d_loss: inf ,  g_loss: inf,  db_loss: 0.3946\n",
      "Epoch : [387] ,  a_loss = 156.0287, d_loss: inf ,  g_loss: inf,  db_loss: 1.2734\n",
      "Epoch : [388] ,  a_loss = 116.6385, d_loss: inf ,  g_loss: inf,  db_loss: 0.0803\n",
      "Epoch : [389] ,  a_loss = 165.6967, d_loss: inf ,  g_loss: inf,  db_loss: 0.4576\n",
      "Epoch : [390] ,  a_loss = 184.8377, d_loss: inf ,  g_loss: inf,  db_loss: 1.3076\n",
      "Epoch : [391] ,  a_loss = 160.2988, d_loss: inf ,  g_loss: inf,  db_loss: 0.7841\n",
      "Epoch : [392] ,  a_loss = 113.2707, d_loss: inf ,  g_loss: inf,  db_loss: 1.2293\n",
      "Epoch : [393] ,  a_loss = 233.2188, d_loss: inf ,  g_loss: inf,  db_loss: 0.2185\n",
      "Epoch : [394] ,  a_loss = 143.8068, d_loss: inf ,  g_loss: inf,  db_loss: 0.3949\n",
      "Epoch : [395] ,  a_loss = 208.7745, d_loss: inf ,  g_loss: inf,  db_loss: 2.1937\n",
      "Epoch : [396] ,  a_loss = 207.3858, d_loss: inf ,  g_loss: inf,  db_loss: 1.2690\n",
      "Epoch : [397] ,  a_loss = 169.4900, d_loss: inf ,  g_loss: inf,  db_loss: 0.0892\n",
      "Epoch : [398] ,  a_loss = 153.1492, d_loss: inf ,  g_loss: inf,  db_loss: 0.2747\n",
      "Epoch : [399] ,  a_loss = 164.5155, d_loss: inf ,  g_loss: inf,  db_loss: 0.2051\n",
      "latent_matrix shape (933704, 10)\n",
      "(933704,)\n"
     ]
    }
   ],
   "source": [
    " \n",
    "run_config = tf.ConfigProto()\n",
    "run_config.gpu_options.per_process_gpu_memory_fraction = 0.333\n",
    "run_config.gpu_options.allow_growth = True\n",
    "\n",
    "checkpoint_dir = path + dataset_name #\"/data/eugene/AAE-20180306-Hemberg/test_checkpoint\"\n",
    "with tf.Session(config = run_config) as sess:\n",
    "\n",
    "    g_h_dim = [g_h_l1, g_h_l2, g_h_l3, g_h_l4]\n",
    "    d_h_dim = [d_h_l1, d_h_l2, d_h_l3, d_h_l4]\n",
    "\n",
    "    if model == 'dra':\n",
    "\n",
    "            test_dra = Test_DRA(\n",
    "                sess,\n",
    "                epoch = epoch,\n",
    "                lr = learning_rate,\n",
    "                beta1 = beta1,\n",
    "                batch_size = batch_size,\n",
    "                X_dim = X_dim, \n",
    "                z_dim = z_dim,\n",
    "                dataset_name = dataset,\n",
    "                checkpoint_dir = checkpoint_dir,\n",
    "                sample_dir = sample_dir,\n",
    "                result_dir = result_dir,\n",
    "                num_layers = n_l,\n",
    "                g_h_dim = g_h_dim[:n_l],\n",
    "                d_h_dim = d_h_dim[:n_l],\n",
    "                gen_activation = actv,\n",
    "                leak = leak,\n",
    "                keep_param = keep,\n",
    "                trans = trans,\n",
    "                is_bn = bn,\n",
    "                g_iter = g_iter,\n",
    "                lam = lam,\n",
    "                sampler = sampler)\n",
    "\n",
    "\n",
    "    # show_all_variables()\n",
    "    if train:\n",
    "        if model == 'dra':\n",
    "            test_dra.train_cluster()           \n",
    "\n",
    "#tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9K2XJ8pcI3A"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
