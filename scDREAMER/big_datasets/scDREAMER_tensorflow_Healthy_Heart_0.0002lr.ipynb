{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ba6OBfX1gnzG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 11:49:49.957734: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-23 11:49:50.787996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1817465/424770243.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# from tensorflow.python import keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;31m# from tensorflow.python.layers import layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/feature_column/feature_column_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# pylint: disable=unused-import,line-too-long,wildcard-import,g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_feature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_tensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msparse_tensor_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# =============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_tf_layers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mInputSpec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtraining_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork_serialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcallbacks_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0mrequests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \"\"\"\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRequestsDependencyWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnectionpool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTTPConnectionPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTTPSConnectionPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection_from_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfilepost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mencode_multipart_formdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpoolmanager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPoolManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProxyManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_from_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msocket\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from .connection import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mBrokenPipeError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTTPConnection\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTTPException\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_proxy_ssl_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Compiled with SSL?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/util/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRetry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from .ssl_ import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mALPN_PROTOCOLS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mHAS_SNI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBRACELESS_IPV6_ADDRZ_RE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIPV4_RE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mSSLContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/util/url.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mIPV4_RE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"^\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mIPV4_PAT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mIPV6_RE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"^\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mIPV6_PAT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mIPV6_ADDRZ_RE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"^\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mIPV6_ADDRZ_PAT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mBRACELESS_IPV6_ADDRZ_RE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"^\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mIPV6_ADDRZ_PAT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mZONE_ID_RE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"(\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mZONE_ID_PAT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mr\")\\]$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/re.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;34m\"Compile a regular expression pattern, returning a Pattern object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpurge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0m\u001b[1;32m    444\u001b[0m                            not nested and not items))\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    832\u001b[0m             sub_verbose = ((verbose or (add_flags & SRE_FLAG_VERBOSE)) and\n\u001b[1;32m    833\u001b[0m                            not (del_flags & SRE_FLAG_VERBOSE))\n\u001b[0;32m--> 834\u001b[0;31m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_verbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnested\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\")\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m                 raise source.error(\"missing ), unterminated subpattern\",\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0m\u001b[1;32m    444\u001b[0m                            not nested and not items))\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    832\u001b[0m             sub_verbose = ((verbose or (add_flags & SRE_FLAG_VERBOSE)) and\n\u001b[1;32m    833\u001b[0m                            not (del_flags & SRE_FLAG_VERBOSE))\n\u001b[0;32m--> 834\u001b[0;31m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_verbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnested\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\")\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m                 raise source.error(\"missing ), unterminated subpattern\",\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0m\u001b[1;32m    444\u001b[0m                            not nested and not items))\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mAT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m                 raise source.error(\"nothing to repeat\",\n\u001b[1;32m    669\u001b[0m                                    source.tell() - here + len(this))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "#import tables\n",
    "import scipy.io\n",
    "from sklearn.decomposition import PCA\n",
    "import pdb\n",
    "\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy import stats \n",
    "from scipy import * \n",
    "import datetime \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf2\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()\n",
    "np.random.seed(0)\n",
    "#tf.set_random_seed(0)\n",
    "tf.set_random_seed(0)\n",
    "random.seed(0)\n",
    "tf2.random.set_seed(0)\n",
    "tf2.keras.utils.set_random_seed(0)\n",
    "#tf.keras.utils.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkVerHg8IS97"
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = \"Healthy_Heart\"\n",
    "category = 147\n",
    "\n",
    "hvg = 2000\n",
    "#hvg = 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mYXBjJYgqqc",
    "outputId": "4e9c43eb-b656-4ba6-fbbc-09658a05db9d"
   },
   "outputs": [],
   "source": [
    "print (tf.__version__) #1.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Go9Bo6ceSDoO",
    "outputId": "d9b3f3d7-c21d-4691-9045-7935729a4a62"
   },
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyIXAPC_VJ0h"
   },
   "outputs": [],
   "source": [
    "#path = '/content/drive/My Drive/Colab Notebooks/Project/advVAE/'\n",
    "path = \"../../\"\n",
    "#path = '/home/ajita/Documents/IITK_AS/DI/SCRNA_Datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_Uk1Y5JXU0O"
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "# Util.py .....\n",
    "def read_mtx(filename, trans_flag = False):\n",
    "     buffer = scipy.io.mmread(filename)\n",
    "\n",
    "     if trans_flag:\n",
    "         print('Transpose Data !')\n",
    "         return buffer.transpose()\n",
    "     else:\n",
    "         return buffer\n",
    "\n",
    "def write_mtx(filename, data):\n",
    "    scipy.io.mmwrite(filename, data)\n",
    "\n",
    "# Leaky Relu\n",
    "def lrelu(x, alpha = 0.2, name='lrelu'):\n",
    "    return tf.maximum(x, alpha*x)\n",
    "\n",
    "def dense(x, inp_dim, out_dim, name = 'dense'):\n",
    "\n",
    "    with tf.variable_scope(name, reuse=None): # earlier only tf\n",
    "        weights = tf.get_variable(\"weights\", shape=[inp_dim, out_dim],\n",
    "                                  initializer = tf2.initializers.GlorotUniform()) # contrib: tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        bias = tf.get_variable(\"bias\", shape=[out_dim], initializer = tf.constant_initializer(0.0))\n",
    "        \n",
    "        # initializer= tf2.initializers.GlorotUniform(); same as Xavier's initializer; tf.contrib.layers.xavier_initializer()    \n",
    "        out = tf.add(tf.matmul(x, weights), bias, name='matmul')\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOjEeW9VCzK0"
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_npy():\n",
    "    batch_info = np.load(path + 'Pancreas_/accessions.npy')\n",
    "    features = np.load(path + 'Pancreas_/features.npy')\n",
    "    labels = np.load(path + 'Pancreas_/labels.npy')\n",
    "\n",
    "    batch_info = np.array([[i] for i in batch_info])\n",
    "\n",
    "    Ann = sc.AnnData(pd.DataFrame(features))\n",
    "    sc.pp.log1p(Ann)\n",
    "\n",
    "    sc.pp.highly_variable_genes(\n",
    "        Ann, \n",
    "        flavor=\"seurat\", \n",
    "        n_top_genes = 2000, # 721\n",
    "        subset=True)\n",
    "\n",
    "    df_final = pd.DataFrame(Ann.X).reset_index(drop = True)\n",
    "    data = df_final[df_final.columns[:2000]].to_numpy()\n",
    "    return data\n",
    "\n",
    "def read_h5ad(data_path, B, C):\n",
    "    Ann = sc.read_h5ad(data_path)\n",
    "    Ann.layers[\"counts\"] = Ann.X.copy()\n",
    "\n",
    "    sc.pp.normalize_total(Ann, target_sum=1e4)\n",
    "    sc.pp.log1p(Ann)\n",
    "    Ann.raw = Ann \n",
    "    \n",
    "    sc.pp.highly_variable_genes(\n",
    "        Ann, \n",
    "        flavor=\"seurat\", \n",
    "        n_top_genes=hvg,\n",
    "        batch_key=B,\n",
    "        subset=True)\n",
    "  \n",
    "    df_final = pd.DataFrame.sparse.from_spmatrix(Ann.X) # Lung, Simulation1, Simulation 2, Heslthy Heart\n",
    "    #df_final = pd.DataFrame(Ann.X) # Immune , Pan\n",
    "\n",
    "    df_final = df_final.reset_index(drop = True)\n",
    "    data = df_final.to_numpy()\n",
    "    labels = Ann.obs[C].to_list()\n",
    "\n",
    "    #AJ: Convert to categorical instead of this...\n",
    "    t_ = Ann.obs[B] #.to_list()\n",
    "    batch_info = np.array([[i] for i in t_]) # for other datasets\n",
    "\n",
    "    #batch_info = np.array(Ann.obs[B].astype(\"category\").reset_index(drop = True)).reshape(-1,1) \n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    #batch_info_enc = enc.fit_transform(batch_info).toarray()\n",
    "  \n",
    "    enc.fit(batch_info.reshape(-1, 1))\n",
    "    batch_info_enc = enc.transform(batch_info.reshape(-1, 1)).toarray()\n",
    "\n",
    "    #print ('batches', batch_info_enc)\n",
    "    #print ('labels:', labels)\n",
    "\n",
    "    #print ('batch info', np.unique(batch_info))\n",
    "    #print ('labels:', labels)\n",
    "    return data, labels, batch_info_enc, batch_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7jU6FDDXhtJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_gene_mtx(dataset_name, transform = True, count = True, actv = 'sig'):\n",
    "\n",
    "    # for CSV files...\n",
    "    if (dataset_name == 'Pancreas'):\n",
    "        df_final = pd.read_csv(path + 'Pancreas' +'/pancreas_mat_processed.csv') #_2000\n",
    "        df_final = df_final.iloc[:, :-1]\n",
    "        data = df_final.to_numpy()\n",
    "        \n",
    "        data_path = path + dataset_name + '/labels.csv' \n",
    "        labels = pd.read_csv(data_path, header = None)\n",
    "       \n",
    "    # for npy format files...\n",
    "    elif (dataset_name in ('Pancreas_', 'Pancreas1', 'Pancreas2', 'Pancreas3', 'Pancreas4', 'Pancreas5', 'Pancreas6', 'Pancreas7')):\n",
    "        data = read_npy()\n",
    "\n",
    "    # for h5ad format files...\n",
    "    elif (dataset_name in ('Lung', 'Immune', 'sim_full', 'Simulation2', 'Pan', 'Human_Retina', \"Healthy_Heart\")):\n",
    "\n",
    "         if (dataset_name == 'Lung'):\n",
    "            data_name = 'Lung/Lung_atlas_public.h5ad' #Lung_atlas_public, Lung\n",
    "            B = 'batch'\n",
    "            C = 'cell_type'\n",
    "\n",
    "         elif (dataset_name == 'Immune'):\n",
    "            data_name = 'Immune/Immune_ALL_human.h5ad' \n",
    "            B = 'batch'\n",
    "            C = 'final_annotation'\n",
    "\n",
    "         elif (dataset_name == 'sim_full'):\n",
    "            data_name = 'Simulation/sim1_full.h5ad' \n",
    "            B = 'Batch'\n",
    "            C = 'Group' \n",
    "            \n",
    "         elif (dataset_name == 'Simulation2'):\n",
    "            data_name = 'Simulation2/sim2.h5ad' \n",
    "            B = 'Batch'\n",
    "            C = 'Group'\n",
    "\n",
    "         elif (dataset_name == \"Pan\"):\n",
    "            data_name = \"Pan/Pancreas.h5ad\"\n",
    "            B = \"tech\"\n",
    "            C = \"celltype\"\n",
    "\n",
    "         elif (dataset_name == \"Human_Retina\"):\n",
    "            data_name = \"Human_Retina/Human_Retina.h5ad\"\n",
    "            B = \"Batch\"\n",
    "            C = \"Subcluster\"\n",
    "            \n",
    "         elif (dataset_name == \"Healthy_Heart\"):\n",
    "            data_name = \"Healthy_Heart/Healthy_human_heart_adata.h5ad\"\n",
    "            B = \"sampleID\"\n",
    "            C = \"celltype\"\n",
    "            \n",
    "         data, labels, batch_info_enc, batch_info = read_h5ad(path + data_name, B, C)\n",
    "         \n",
    "    else:\n",
    "\n",
    "        data_path = path + dataset_name +'/sub_set-720.mtx'\n",
    "        data = scipy.io.mmread(data_path) #read_mtx\n",
    "       \n",
    "        data_path = path + dataset_name + '/labels.txt'\n",
    "        labels = np.loadtxt(data_path)\n",
    "        \n",
    "    '''\n",
    "    if dataset_name in ('Pancreas'):\n",
    "        data_path = path + dataset_name + '/tech.csv' \n",
    "        t_labels = pd.read_csv(data_path, header = None)\n",
    "        t_labels = t_labels.reset_index(drop = True) \n",
    "        batch_info = t_labels.to_numpy()\n",
    "        print ('batch info', batch_info)\n",
    "\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        #batch_info_enc = enc.fit_transform(batch_info).toarray()\n",
    "  \n",
    "        enc.fit(batch_info.reshape(-1, 1))\n",
    "        batch_info_enc = enc.transform(batch_info.reshape(-1, 1)).toarray()\n",
    "    ''' \n",
    "    print ('Shape of data is: ', data.shape)\n",
    "\n",
    "    if transform:\n",
    "        data = transform_01_gene_input(data)\n",
    "        print('Data Transformed, entries in [0, 1] !'.format(data_path))\n",
    "    else:\n",
    "        if count == False:\n",
    "            data = np.log2(data+1)\n",
    "\n",
    "            if actv == 'lin':\n",
    "                scale = 1.0\n",
    "            else:\n",
    "                scale = np.max(data)\n",
    "            data = data / scale           \n",
    "\n",
    "    ord_enc = LabelEncoder()\n",
    "    labels  = ord_enc.fit_transform(labels)\n",
    "    print ('here', labels)\n",
    "\n",
    "    unique, counts = np.unique(labels, return_counts = True)\n",
    "    dict(zip(unique, counts))\n",
    "    \n",
    "    total_size = data.shape[0]\n",
    "\n",
    "    if count == False:\n",
    "        return data, data, scale, labels, labels, batch_info_enc, batch_info_enc, batch_info\n",
    "\n",
    "    return data, data, labels, labels, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8MDNCdZC9Qe"
   },
   "source": [
    "Class **Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnC_XfBeHHq-"
   },
   "source": [
    "Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMmPH2fy_G1j"
   },
   "outputs": [],
   "source": [
    "# Class Functions:\n",
    "def build_model(self):\n",
    "\n",
    "    self.N_batch = category\n",
    "    self.x_input = tf.placeholder(dtype=tf.float32, shape=[None, self.X_dim], name='Input')\n",
    "    #self.x_input_ = tf.placeholder(dtype=tf.float32, shape=[None, self.X_dim], name='Input')\n",
    "    self.x_target = tf.placeholder(dtype=tf.float32, shape=[None, self.X_dim], name='Target')\n",
    "    self.batch_input = tf.placeholder(dtype = tf.float32, shape=[None, self.N_batch], name='batch_input') # 6, 3\n",
    "    #self.batch_input_ = tf.placeholder(dtype = tf.float32, shape=[None, self.N_batch], name='batch_input')\n",
    "    \n",
    "    self.keep_prob = tf.placeholder(dtype=tf.float32, name = 'keep_prob')\n",
    "    self.real_distribution = tf.placeholder(dtype=tf.float32, shape=[None, self.z_dim], name='Real_distribution')\n",
    "    self.kl_scale = tf.placeholder(tf.float32, (), name='kl_scale')\n",
    "    \n",
    "    self.kl_scale = 0.001 # 0.01, 0, 0.001\n",
    "    self.dropout_rate = 0.1\n",
    "    self.training_phase = True \n",
    "    self.n_layers = self.num_layers \n",
    "    self.n_latent = self.z_dim\n",
    "    \n",
    "    # AJ\n",
    "    self.enc_input = tf.concat([self.x_input, self.batch_input],1)\n",
    "    #self.enc_input_ = tf.concat([self.x_input_, self.batch_input_],1)\n",
    "    print('encoder input shape ',self.enc_input)\n",
    "\n",
    "    # AJ: Encoder output...\n",
    "    self.encoder_output, self.z_post_m, self.z_post_v, self.l_post_m, self.l_post_v = self.encoder(self.enc_input) # self.x_input\n",
    "    #self.encoder_output_, self.z_post_m_, self.z_post_v_, self.l_post_m_, self.l_post_v_ = self.encoder(self.enc_input_, reuse = True) \n",
    "\n",
    "    self.expression = self.x_input               \n",
    "    self.proj = tf.placeholder(dtype=tf.float32, shape=[None, self.X_dim], name='projection')\n",
    "  \n",
    "    log_library_size = np.log(np.sum(self.data_train, axis=1)) \n",
    "    mean, variance = np.mean(log_library_size), np.var(log_library_size)\n",
    "\n",
    "    library_size_mean = mean\n",
    "    library_size_variance = variance\n",
    "    self.library_size_mean = tf.to_float(tf.constant(library_size_mean))\n",
    "    self.library_size_variance = tf.to_float(tf.constant(library_size_variance))    \n",
    "    \n",
    "    self.z = self.sample_gaussian(self.z_post_m, self.z_post_v) \n",
    "    #self.z_ = self.sample_gaussian(self.z_post_m_, self.z_post_v_) \n",
    "\n",
    "    self.library = self.sample_gaussian(self.l_post_m, self.l_post_v)\n",
    "    \n",
    "    # AJ\n",
    "    #self.decoder_output = self.decoder(self.z)  \n",
    "    print('decoder input shape ',tf.concat([self.z, self.batch_input],1))\n",
    "\n",
    "    self.decoder_output = self.decoder(tf.concat([self.z, self.batch_input],1))             \n",
    "    self.n_input = self.expression.get_shape().as_list()[1]\n",
    "  \n",
    "    self.x_post_scale = tf.nn.softmax(dense(self.decoder_output, self.g_h_dim[0], self.n_input, name='dec_x_post_scale')) \n",
    "    self.x_post_r = tf.Variable(tf.random_normal([self.n_input]), name=\"dec_x_post_r\")           \n",
    "    self.x_post_rate = tf.exp(self.library) * self.x_post_scale\n",
    "    self.x_post_dropout = dense(self.decoder_output, self.g_h_dim[0], self.n_input, name='dec_x_post_dropout') \n",
    "        \n",
    "    local_dispersion = tf.exp(self.x_post_r)            \n",
    "    local_l_mean = self.library_size_mean\n",
    "    local_l_variance = self.library_size_variance\n",
    "\n",
    "    self.decoder_output2 = tf.nn.sigmoid(dense(self.decoder_output, self.g_h_dim[0], self.X_dim, 'dec_output2'))  \n",
    "    \n",
    "    # Discriminator D1....\n",
    "    self.dis_real_logit = self.discriminator(self.real_distribution, self.z_dim) # random z from Gaussina distribution ...\n",
    "    self.dis_fake_logit = self.discriminator(self.z, self.z_dim, reuse=True)  # z distribution coming from encoder...network \n",
    "    \n",
    "    # Discriminator D2\n",
    "    self.dis2_real_logit = self.discriminator2(self.x_target, self.X_dim) # True data\n",
    "    self.dis2_fake_logit = self.discriminator2(self.decoder_output2, self.X_dim, reuse=True) # from decoder network\n",
    "    \n",
    "    # Discriminator D_batch : discriminate between different batches info\n",
    "    # pass the encoded data; self.x_target, self.X_dim\n",
    "    self.disb_real_logit = self.discriminatorB(self.z, self.z_dim) # True data\n",
    "    \n",
    "    # Reconstruction loss \n",
    "    capL = 1e-4 #1e-8\n",
    "    capU = 1e4 #1e8\n",
    "\n",
    "    #recon_loss = 0\n",
    "    recon_loss = self.zinb_model(self.expression, self.x_post_rate, local_dispersion, self.x_post_dropout)\n",
    "    #recon_loss = tf.reduce_mean(tf.square(tf.subtract( self.decoder_output2, self.x_input)))\n",
    "    \n",
    "    self.kl_gauss_l = 0.5 * tf.reduce_sum(- tf.log(tf.math.minimum(tf.math.maximum(self.l_post_v, capL), capU))  \\\n",
    "                                      + self.l_post_v/local_l_variance \\\n",
    "                                      + tf.square(self.l_post_m - local_l_mean)/local_l_variance  \\\n",
    "                                      + tf.log(tf.math.minimum(tf.math.maximum(local_l_variance, capL), capU)) - 1, 1)\n",
    "\n",
    "    self.kl_gauss_z = 0.5 * tf.reduce_sum(- tf.log(tf.math.minimum(tf.math.maximum(self.z_post_v, capL), capU)) + self.z_post_v + tf.square(self.z_post_m) - 1, 1)\n",
    "\n",
    "    print ('KL gaussian z', self.kl_gauss_z)\n",
    "    print ('KL gaussian l', self.kl_gauss_l)\n",
    "\n",
    "    # Evidence lower bound - ELBO : KLscale to prevent posterior collapse...\n",
    "    #self.ELBO_gauss = tf.reduce_mean(recon_loss - self.kl_gauss_l - self.kl_scale * self.kl_gauss_z) - tf.reduce_sum(tf.pow(self.z - self.z_, 2)) \n",
    "    self.ELBO_gauss = tf.reduce_mean(recon_loss - self.kl_scale*self.kl_gauss_l - self.kl_scale*self.kl_gauss_z) \n",
    "    #- self.kl_scale*tf.reduce_sum(tf.pow(self.z - self.z_, 2)) \n",
    "\n",
    "    #tf.reduce_sum(tf.math.sqrt(self.z - self.z_)) #tf.reduce_sum(tf.pow(self.z - self.z_, 2))\n",
    "    \n",
    "    # - is added to ELBO because we maximize the ELBO expression & maximize classifier cross entropy loss -> -loss minimize cross entropy\n",
    "    self.autoencoder_loss = - self.ELBO_gauss - tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.disb_real_logit, labels = self.batch_input)) - tf.log(tf.math.minimum(tf.math.maximum(tf.reduce_sum(tf.sqrt(tf.abs(self.dis2_real_logit/tf.reduce_sum(self.dis2_real_logit)* self.dis2_fake_logit/tf.reduce_sum(self.dis2_fake_logit)))), capL), capU))                    \n",
    "          \n",
    "    # Discriminator D1: minimize distance   min max objective function - BD distance between z (random sample) and generated z sample from encoder\n",
    "    self.dis_loss = - tf.log(tf.math.minimum(tf.math.maximum(tf.reduce_sum(tf.sqrt(tf.abs(self.dis_real_logit/tf.reduce_sum(self.dis_real_logit)\n",
    "                                        * self.dis_fake_logit/tf.reduce_sum(self.dis_fake_logit)))), capL), capU)) \n",
    "            \n",
    "    # Discriminator D2: minimize distance min max objective function - BD distance between X(generated sample) and X input \n",
    "    self.dis2_loss = tf.log(tf.math.minimum(tf.math.maximum(tf.reduce_sum(tf.sqrt(tf.abs(self.dis2_real_logit/tf.reduce_sum(self.dis2_real_logit)\n",
    "                                        * self.dis2_fake_logit/tf.reduce_sum(self.dis2_fake_logit)))), capL), capU)) # epsilon added to avoid Nan\n",
    "    \n",
    "    # Generator loss - D(z) {D(E(X_real))}  - D1 label will be 1; they are trying to maximize the \n",
    "    self.generator_loss = - tf.log(tf.math.minimum(tf.math.maximum(tf.reduce_sum(tf.sqrt(tf.abs(self.dis_fake_logit/tf.reduce_sum(self.dis_fake_logit)) )), capL), capU)) \n",
    "\n",
    "            \n",
    "    # 27Apr: AJ : minimize binary cross entropy     \n",
    "    self.disb_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.disb_real_logit, labels = self.batch_input)) #self.batch_input\n",
    "    \n",
    "    t_vars = tf.trainable_variables()\n",
    "    self.dis_vars = [var for var in t_vars if 'dis_' in var.name]\n",
    "    self.gen_vars = [var for var in t_vars if 'enc_' in var.name or 'dec_' in var.name] #AS:2109\n",
    "\n",
    "    # Discriminator D2\n",
    "    self.dis2_vars = [var for var in t_vars if 'dis2_' in var.name] #or 'enc_' in var.name or 'dec_' in var.name]\n",
    "    \n",
    "    # Discriminator DB: AJ\n",
    "    #self.disb_vars = [var for var in t_vars if 'disb_' in var.name]\n",
    "    self.disb_vars = [var for var in t_vars if 'disb_' in var.name ]\n",
    "\n",
    "    self.saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJ0gZUTxHJnd"
   },
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAL6VTFuF1cD"
   },
   "outputs": [],
   "source": [
    "def train_cluster(self):\n",
    "\n",
    "    print('Cluster DRA on DataSet {} ... '.format(self.dataset_name))\n",
    "\n",
    "    #tf.train.Optimizer\n",
    "    #autoencoder_optimizer = tf.keras.optimizers.SGD(learning_rate=0.1).minimize(self.autoencoder_loss, var_list = self.gen_vars)\n",
    "    #autoencoder_optimizer = tf.train.GradientDescentOptimizer(learning_rate = self.lr).minimize(self.autoencoder_loss)\n",
    "\n",
    "    # learning_rate=self.lr,beta1=self.beta1 #0.0002, 0.0001\n",
    "    autoencoder_optimizer = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=self.beta1).minimize(self.autoencoder_loss) # self.disb_vars, var_list = self.gen_vars\n",
    "\n",
    "    # Not used at the moment....\n",
    "    #autoencoder_optimizer2 = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "    #                                               beta1=self.beta1).minimize( self.autoencoder_loss) # self.disb_vars\n",
    "\n",
    "    discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "                                                      beta1=self.beta1).minimize(self.dis_loss,\n",
    "                                                                                  var_list=self.dis_vars)\n",
    "    generator_optimizer = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "                                                  beta1=self.beta1).minimize(self.generator_loss,var_list=self.gen_vars)\n",
    "    # Discriminator D2\n",
    "    discriminator2_optimizer = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "                                                      beta1=self.beta1).minimize(self.dis2_loss,\n",
    "                                                                                  var_list=self.dis2_vars)\n",
    "    \n",
    "    # Discriminator batch: Classifier....\n",
    "    \n",
    "    discriminatorb_optimizer = tf.train.AdamOptimizer(learning_rate = self.lr,\n",
    "                                              beta1 = self.beta1).minimize(self.disb_loss, var_list = self.disb_vars)\n",
    "    \n",
    "    self.sess.run(tf.global_variables_initializer())\n",
    "    a_loss_epoch = []\n",
    "    d_loss_epoch = []\n",
    "    g_loss_epoch = []\n",
    "    d2_loss_epoch = [] # Discriminator D2\n",
    "    db_loss_epoch = [] # Discriminator batch\n",
    "\n",
    "    control = 3 # Generator is updated twice for each Discriminator D1 update\n",
    "\n",
    "    num_batch_iter = self.total_size // self.batch_size\n",
    "    \n",
    "    for ep in range(self.epoch):\n",
    "    #for it in range(num_batch_iter):\n",
    "        d_loss_curr = g_loss_curr = a_loss_curr = np.inf\n",
    "        self._is_train = True\n",
    "\n",
    "        # AJ: oncatenating x_train and batch info for batches preparation\n",
    "        X = np.concatenate((self.data_train, self.batch_train), axis = 1)\n",
    "\n",
    "\n",
    "        for it in range(num_batch_iter):\n",
    "\n",
    "            batch_x, X_ = self.next_batch(self.data_train, self.batch_train, self.train_size)\n",
    "            \"\"\"\n",
    "            df = pd.DataFrame(batch_x)\n",
    "            col = df.columns[:100] # random col generate\n",
    "            for c in col: #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "                df[c] = 0\n",
    "            \n",
    "            batch_x_ = df.to_numpy()\n",
    "            \"\"\"\n",
    "            # Random Sampling for the batch size...\n",
    "            batch_z_real_dist = self.sample_Z(self.batch_size, self.z_dim)\n",
    "\n",
    "            _, a_loss_curr = self.sess.run([autoencoder_optimizer, self.autoencoder_loss],\n",
    "                                            feed_dict={self.x_input: batch_x, self.x_target: batch_x, \n",
    "                                                       #self.x_input_: batch_x_, self.batch_input_: X_,\n",
    "                                                      self.batch_input: X_, # batch_b\n",
    "                                                      self.keep_prob: self.keep_param}) \n",
    "  \n",
    "                \n",
    "            #if np.mod(it, control) == 0: \n",
    "\n",
    "            '''\n",
    "            _, d_loss_curr = self.sess.run([discriminator_optimizer, self.dis_loss],\n",
    "                feed_dict={self.x_input: batch_x,\n",
    "                            self.batch_input: X_,\n",
    "                self.real_distribution: batch_z_real_dist,\n",
    "                self.x_input_: batch_x_, self.batch_input_: X_,\n",
    "                self.keep_prob: self.keep_param})    \n",
    "            '''\n",
    "            \n",
    "            '''                                          \n",
    "            else: \n",
    "                \n",
    "                _, g_loss_curr = self.sess.run([generator_optimizer, self.generator_loss], \n",
    "                    feed_dict={self.x_input: batch_x, self.x_target: batch_x, self.keep_prob: self.keep_param, \n",
    "                               self.x_input_: batch_x_, self.batch_input_: X_,\n",
    "                               self.batch_input: X_, self.real_distribution: batch_z_real_dist}) #self.generator_loss\n",
    "            '''\n",
    "            \n",
    "            # AJ: Count here, D2 is taking true data only.\n",
    "            _, d2_loss_curr = self.sess.run([discriminator2_optimizer, self.dis2_loss],\n",
    "                        feed_dict={self.x_input: batch_x,\n",
    "                        self.x_target: batch_x,\n",
    "                        self.batch_input: X_,\n",
    "                        self.real_distribution: batch_z_real_dist,\n",
    "                        #self.x_input_: batch_x_, self.batch_input_: X_,\n",
    "                        self.keep_prob: self.keep_param}) \n",
    "            \n",
    "            _, db_loss_curr = self.sess.run([discriminatorb_optimizer, self.disb_loss],\n",
    "                feed_dict={self.x_input: batch_x,\n",
    "                            self.batch_input: X_,\n",
    "                self.real_distribution: batch_z_real_dist,\n",
    "                self.batch_input: X_, # batch_b\n",
    "                #self.x_input_: batch_x_, self.batch_input_: X_,\n",
    "                self.keep_prob: self.keep_param})\n",
    "            \n",
    "            \n",
    "        #db_loss_curr = 0\n",
    "        print(\"Epoch : [%d] ,  a_loss = %.4f, d_loss: %.4f ,  g_loss: %.4f,  db_loss: %.4f\" \n",
    "              % (ep, a_loss_curr, d_loss_curr, g_loss_curr,db_loss_curr))\n",
    "        \n",
    "        \n",
    "        self._is_train = False # enables false after 1st iterations only...to make training process fast\n",
    "\n",
    "        if (np.isnan(a_loss_curr) or np.isnan(d_loss_curr) or np.isnan(g_loss_curr) or np.isnan(db_loss_curr)): # np.isnan(d2_loss_curr)\n",
    "          a_loss_curr = 0\n",
    "          d_loss_curr = 0\n",
    "          g_loss_curr = 0\n",
    "          d2_loss_curr = 0\n",
    "          db_loss_curr = 0\n",
    "          break\n",
    "\n",
    "        #self.x_target: batch_x,\n",
    "        a_loss_epoch.append(a_loss_curr) # total loss getting appended \n",
    "        d_loss_epoch.append(d_loss_curr)\n",
    "        g_loss_epoch.append(g_loss_curr)\n",
    "        #d2_loss_epoch.append(d2_loss_curr)\n",
    "        db_loss_epoch.append(db_loss_curr)\n",
    "        \n",
    "        if (ep % 100 == 0 and ep != 0):\n",
    "            self.eval_cluster_on_test_(ep)\n",
    "\n",
    "    self.eval_cluster_on_test(ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_t3LUqLHLz_"
   },
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYHxxwi7GL2V"
   },
   "outputs": [],
   "source": [
    "# reuse = False\n",
    "\n",
    "def encoder(self, x, reuse = False):\n",
    "    \"\"\"\n",
    "    Encode part of the autoencoder.\n",
    "    :param x: input to the autoencoder\n",
    "    :param reuse: True -> Reuse the encoder variables, False -> Create or search of variables before creating\n",
    "    :return: tensor which is the hidden latent variable of the autoencoder.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope('Encoder') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        if self.is_bn:\n",
    "            h = tf.layers.batch_normalization(\n",
    "\n",
    "                lrelu(dense(x, self.X_dim + self.N_batch, self.g_h_dim[0], name='enc_h0_lin'), alpha=self.leak),\n",
    "                training=self._is_train, name='enc_bn0')\n",
    "                \n",
    "            for i in range(1, self.num_layers):\n",
    "                h = tf.layers.batch_normalization(\n",
    "\n",
    "                    lrelu(dense(h, self.g_h_dim[i - 1], self.g_h_dim[i], name='enc_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak),\n",
    "                    training=self._is_train, name='enc_bn' + str(i))                    \n",
    "\n",
    "            z_post_m = dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_z_post_m' + str(self.num_layers) + '_lin')                \n",
    "            z_post_v = tf.exp(dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_z_post_v' + str(self.num_layers) + '_lin'))              \n",
    "            \n",
    "            h = tf.nn.relu(dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_h' + str(self.num_layers) + '_lin'))\n",
    "\n",
    "            l_post_m = dense(h, self.z_dim, 1, name='enc_l_post_m' + str(self.num_layers) + '_lin')                            \n",
    "            l_post_v = tf.exp(dense(h, self.z_dim, 1, name='enc_l_post_v' + str(self.num_layers) + '_lin')) \n",
    "            \n",
    "\n",
    "        else:\n",
    "\n",
    "            h = tf.nn.dropout(lrelu(dense(x, self.X_dim + self.N_batch, self.g_h_dim[0], name='enc_h0_lin'), alpha=self.leak),\n",
    "                              keep_prob=self.keep_prob)                \n",
    "            \n",
    "            for i in range(1, self.num_layers):\n",
    "                \n",
    "                h = tf.nn.dropout(lrelu(dense(h, self.g_h_dim[i - 1], self.g_h_dim[i], name='enc_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak), keep_prob=self.keep_prob)                    \n",
    "\n",
    "            z_post_m = dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_z_post_m' + str(self.num_layers) + '_lin')                \n",
    "            z_post_v = tf.exp(dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_z_post_v' + str(self.num_layers) + '_lin'))\n",
    "            \n",
    "            \n",
    "            h = tf.nn.relu(dense(h, self.g_h_dim[self.num_layers - 1], self.z_dim, name='enc_h' + str(self.num_layers) + '_lin'))\n",
    "                      \n",
    "\n",
    "            l_post_m = dense(h, self.z_dim, 1, name='enc_l_post_m' + str(self.num_layers) + '_lin')                             \n",
    "            l_post_v = tf.exp(dense(h, self.z_dim, 1, name='enc_l_post_v' + str(self.num_layers) + '_lin'))                                          \n",
    "                        \n",
    "        return h, z_post_m, z_post_v, l_post_m, l_post_v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXs3ma44HNlC"
   },
   "source": [
    "Discriminator - Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3agMa0XG4nW"
   },
   "outputs": [],
   "source": [
    "def discriminator(self, z, z_dim, reuse=False):    \n",
    "    \"\"\"\n",
    "    Discriminator that is used to match the posterior distribution with a given prior distribution.\n",
    "    :param z: tensor of shape [batch_size, z_dim]\n",
    "    :param reuse: True -> Reuse the discriminator variables,\n",
    "                  False -> Create or search of variables before creating\n",
    "    :return: tensor of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Discriminator') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        if self.is_bn:\n",
    "\n",
    "            h = tf.layers.batch_normalization(\n",
    "                lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='dis_h' + str(self.num_layers-1) + '_lin'),      \n",
    "                      alpha=self.leak),\n",
    "                training=self._is_train, name='dis_bn' + str(self.num_layers-1))\n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                h = tf.layers.batch_normalization(\n",
    "                    lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='dis_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak),\n",
    "                    training=self._is_train, name='dis_bn' + str(i))\n",
    "\n",
    "        else:\n",
    "\n",
    "            h = tf.nn.dropout(\n",
    "                lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='dis_h' + str(self.num_layers-1) + '_lin'),\n",
    "                      alpha=self.leak),\n",
    "                keep_prob=self.keep_prob)\n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                h = tf.nn.dropout(\n",
    "                    lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='dis_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak), keep_prob=self.keep_prob)\n",
    "\n",
    "        output = dense(h, self.d_h_dim[0], 1, name='dis_output')\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWo-DCr1HRsG"
   },
   "source": [
    "Discriminator - X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vh_WY-GjHC2s"
   },
   "outputs": [],
   "source": [
    "def discriminator2(self, z, z_dim, reuse=False):    \n",
    "    \"\"\"\n",
    "    Discriminator that is used to match the posterior distribution with a given prior distribution.\n",
    "    :param z: tensor of shape [batch_size, z_dim]\n",
    "    :param reuse: True -> Reuse the discriminator variables,\n",
    "                  False -> Create or search of variables before creating\n",
    "    :return: tensor of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Discriminator2') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        if self.is_bn:\n",
    "\n",
    "            h = tf.layers.batch_normalization(\n",
    "                lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='dis2_h' + str(self.num_layers-1) + '_lin'),      \n",
    "                      alpha=self.leak),\n",
    "                training=self._is_train, name='dis2_bn' + str(self.num_layers-1))\n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                h = tf.layers.batch_normalization(\n",
    "                    lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='dis2_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak),\n",
    "                    training=self._is_train, name='dis2_bn' + str(i))\n",
    "\n",
    "        else:\n",
    "\n",
    "            h = tf.nn.dropout(\n",
    "                lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='dis2_h' + str(self.num_layers-1) + '_lin'),\n",
    "                      alpha=self.leak),\n",
    "                keep_prob=self.keep_prob)\n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                h = tf.nn.dropout(\n",
    "                    lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='dis2_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak), keep_prob=self.keep_prob)\n",
    "\n",
    "        output = dense(h, self.d_h_dim[0], 1, name='dis2_output')\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTzTzd0CHUxH"
   },
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTTvlUYpGzoM"
   },
   "outputs": [],
   "source": [
    "def decoder(self, z, reuse=False):\n",
    "    \"\"\"\n",
    "    Decoder part of the autoencoder.\n",
    "    :param z: input to the decoder\n",
    "    :param reuse: True -> Reuse the decoder variables, False -> Create or search of variables before creating\n",
    "    :return: tensor which should ideally be the input given to the encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope('Decoder') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        if self.is_bn:\n",
    "\n",
    "            h = tf.layers.batch_normalization(\n",
    "              \n",
    "                lrelu(dense(z , self.z_dim + self.N_batch, self.g_h_dim[self.num_layers-1], name='dec_h' + str(self.num_layers-1) + '_lin'),\n",
    "                      alpha=self.leak),                   \n",
    "                training=self._is_train, name='dec_bn' + str(self.num_layers-1))\n",
    "            for i in range(self.num_layers-2, -1,-1):\n",
    "                h = tf.layers.batch_normalization(\n",
    "\n",
    "                    lrelu(dense(h, self.g_h_dim[i + 1], self.g_h_dim[i], name='dec_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak),                        \n",
    "                    training=self._is_train, name='dec_bn' + str(i))\n",
    "        else:\n",
    "            h = tf.nn.dropout(lrelu(dense(z, self.z_dim + self.N_batch, self.g_h_dim[self.num_layers-1], name='dec_h' + str(self.num_layers-1) + '_lin'),\n",
    "                                    alpha=self.leak),                                  \n",
    "                              keep_prob=self.keep_prob)\n",
    "            for i in range(self.num_layers-2, -1, -1):\n",
    "                h = tf.nn.dropout(\n",
    "                    lrelu(dense(h, self.g_h_dim[i + 1], self.g_h_dim[i], name='dec_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak), keep_prob=self.keep_prob)\n",
    "            \n",
    "        return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnxjzgINHWP5"
   },
   "source": [
    "Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nuIBTqaG1Rl"
   },
   "outputs": [],
   "source": [
    "def discriminatorB(self, z, z_dim, reuse = False):    \n",
    "    \n",
    "    \"\"\"\n",
    "    Discriminator takes the real data and try ti differentiate between different batches\n",
    "    :param x: tensor of shape [batch_size, x_dim]\n",
    "    :param batch: tensor of shape [batch_size] -> batchinfo of the train data\n",
    "    :param reuse: True -> Reuse the discriminator variables,False -> Create or search of variables before creating\n",
    "    :return: tensor of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    '''\n",
    "    x_ = pd.DataFrame(x.numpy())\n",
    "    x_ = pd.concat([x_, batch], axis = 1)\n",
    "    x = torch.tensor(x.values)\n",
    "    x_dim = torch.tensor(721)\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope('discriminatorB') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        if self.is_bn:\n",
    "\n",
    "            h = tf.layers.batch_normalization(\n",
    "                lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='disb_h' + \n",
    "                            str(self.num_layers-1) + '_lin'), alpha = self.leak),\n",
    "                            training=self._is_train, name='disb_bn' + str(self.num_layers-1))\n",
    "            \n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                h = tf.layers.batch_normalization(\n",
    "                    lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='disb_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak),\n",
    "                    training=self._is_train, name='disb_bn' + str(i))\n",
    "\n",
    "        else:\n",
    "\n",
    "            h = tf.nn.dropout(\n",
    "                lrelu(dense(z, z_dim, self.d_h_dim[self.num_layers - 1], name='disb_h' + str(self.num_layers-1) + '_lin'),\n",
    "                      alpha=self.leak),\n",
    "                keep_prob=self.keep_prob)\n",
    "            \n",
    "            for i in range(self.num_layers - 2, -1, -1):\n",
    "                h = tf.nn.dropout(\n",
    "                    lrelu(dense(h, self.d_h_dim[i + 1], self.d_h_dim[i], name='disb_h' + str(i) + '_lin'),\n",
    "                          alpha=self.leak), keep_prob=self.keep_prob)\n",
    "\n",
    "        # AJ 6 in pace of 1 , 3 for pancreas \n",
    "        output = dense(h, self.d_h_dim[0], self.N_batch, name='disb_output')\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3bSCL1cHqFI"
   },
   "outputs": [],
   "source": [
    "#AJ: 20 may\n",
    "# Zero-inflated negative binomial (ZINB) model is for modeling count variables with excessive zeros and it is usually for overdispersed count outcome variables.\n",
    "def zinb_model(self, x, mean, inverse_dispersion, logit, eps=1e-4): \n",
    "\n",
    "    # 1e8 should be of same dimensions as other parameters....                 \n",
    "    expr_non_zero = - tf.nn.softplus(- logit) \\\n",
    "                    + tf.log(inverse_dispersion + eps) * inverse_dispersion \\\n",
    "                    - tf.log(inverse_dispersion + mean + eps) * inverse_dispersion \\\n",
    "                    - x * tf.log(inverse_dispersion + mean + eps) \\\n",
    "                    + x * tf.log(mean + eps) \\\n",
    "                    - tf.lgamma(x + 1) \\\n",
    "                    + tf.lgamma(x + inverse_dispersion) \\\n",
    "                    - tf.lgamma(inverse_dispersion) \\\n",
    "                    - logit \n",
    "    \n",
    "    expr_zero = - tf.nn.softplus( - logit) \\\n",
    "                + tf.nn.softplus(- logit + tf.log(inverse_dispersion + eps) * inverse_dispersion \\\n",
    "                                  - tf.log(inverse_dispersion + mean + eps) * inverse_dispersion) \n",
    "\n",
    "    template = tf.cast(tf.less(x, eps), tf.float32)\n",
    "    expr =  tf.multiply(template, expr_zero) + tf.multiply(1 - template, expr_non_zero)\n",
    "    return tf.reduce_sum(expr, axis=-1)\n",
    "\n",
    "def eval_cluster_on_test_(self, epoch):\n",
    "\n",
    "    # Embedding points in the test data to the latent space\n",
    "    inp_encoder = self.data_test\n",
    "    labels = self.labels_test\n",
    "    batch_label = self.batch_test\n",
    "            \n",
    "    latent_matrix = self.sess.run(self.z, feed_dict = {self.x_input: inp_encoder, self.batch_input: batch_label, self.keep_prob: 1.0})\n",
    "    \n",
    "    print ('latent_matrix shape', latent_matrix.shape)\n",
    "    print (labels.shape)\n",
    "    \n",
    "    Ann = sc.AnnData(inp_encoder)\n",
    "    Ann.obsm['final_embeddings'] = latent_matrix\n",
    "    Ann.obs['group'] = labels.astype(str)\n",
    "    \n",
    "    sc.pp.neighbors(Ann, use_rep = 'final_embeddings') #use_rep = 'final_embeddings'\n",
    "    sc.tl.umap(Ann)\n",
    "    img = sc.pl.umap(Ann, color = 'group', frameon = False) # cells\n",
    "    print(img)\n",
    "    \n",
    "    np.savetxt(path + self.dataset_name + '/latent_matrix_ccc' + str(epoch) +'.csv', latent_matrix, delimiter=\",\")\n",
    "    \n",
    "    Ann.obs['batch'] = self.batch_info.astype(str)\n",
    "    img2 = sc.pl.umap(Ann, color = 'batch', frameon = False)\n",
    "    print(img2)\n",
    "\n",
    "    K = np.size(np.unique(labels))   \n",
    "    kmeans = KMeans(n_clusters=K, random_state=0).fit(latent_matrix)\n",
    "    y_pred = kmeans.labels_\n",
    "\n",
    "    print('Computing NMI ...')\n",
    "    NMI = nmi(labels.flatten(), y_pred.flatten())\n",
    "    print('Done !')\n",
    "\n",
    "    print('NMI = {}'. \n",
    "          format(NMI)) \n",
    "    \n",
    "def eval_cluster_on_test(self, epoch):\n",
    "\n",
    "    # Embedding points in the test data to the latent space\n",
    "    inp_encoder = self.data_test\n",
    "    labels = self.labels_test\n",
    "    batch_label = self.batch_test\n",
    "            \n",
    "    latent_matrix = self.sess.run(self.z, feed_dict = {self.x_input: inp_encoder, self.batch_input: batch_label, self.keep_prob: 1.0})\n",
    "    \n",
    "    print ('latent_matrix shape', latent_matrix.shape)\n",
    "    print (labels.shape)\n",
    "    \n",
    "    Ann = sc.AnnData(inp_encoder)\n",
    "    Ann.obsm['final_embeddings'] = latent_matrix\n",
    "    Ann.obs['group'] = labels.astype(str)\n",
    "    \n",
    "    sc.pp.neighbors(Ann, use_rep = 'final_embeddings') #use_rep = 'final_embeddings'\n",
    "    sc.tl.umap(Ann)\n",
    "    img = sc.pl.umap(Ann, color = 'group', frameon = False) # cells\n",
    "    print(img)\n",
    "    \n",
    "    np.savetxt(path + self.dataset_name + '/latent_matrix_ccc' + str(epoch) +'.csv', latent_matrix, delimiter=\",\")\n",
    "    \n",
    "    Ann.obs['batch'] = self.batch_info.astype(str)\n",
    "    img2 = sc.pl.umap(Ann, color = 'batch', frameon = False)\n",
    "    print(img2)\n",
    "\n",
    "    K = np.size(np.unique(labels))   \n",
    "    kmeans = KMeans(n_clusters=K, random_state=0).fit(latent_matrix)\n",
    "    y_pred = kmeans.labels_\n",
    "\n",
    "    print('Computing NMI ...')\n",
    "    NMI = nmi(labels.flatten(), y_pred.flatten())\n",
    "    print('Done !')\n",
    "\n",
    "    print('NMI = {}'. \n",
    "          format(NMI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9t6iZZZYSw4"
   },
   "outputs": [],
   "source": [
    "# DRA.py\n",
    "\n",
    "class Test_DRA(object):\n",
    "    def __init__(self, sess, epoch = 200, lr=0.0001, beta1=0.5, batch_size=128, X_dim=720, z_dim=10, dataset_name='mnist',\n",
    "                 checkpoint_dir='checkpoint', sample_dir='samples', result_dir = 'result', num_layers = 2, g_h_dim = None,\n",
    "                 d_h_dim = None, gen_activation = 'sig', leak = 0.2, keep_param = 1.0, trans = 'sparse',is_bn = False,\n",
    "                 g_iter = 2, lam=10.0, sampler = 'uniform'):\n",
    "\n",
    "        self.sess = sess\n",
    "        self.epoch = epoch\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.batch_size = batch_size\n",
    "        self.X_dim = X_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.dataset_name = dataset_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.sample_dir = sample_dir\n",
    "        self.result_dir = result_dir\n",
    "        self.num_layers = num_layers\n",
    "        self.g_h_dim = g_h_dim  # Fully connected layers for Generator\n",
    "        self.d_h_dim = d_h_dim  # Fully connected layers for Discriminator\n",
    "        self.gen_activation = gen_activation\n",
    "        self.leak = leak\n",
    "        self.keep_param = keep_param\n",
    "        self.trans = trans\n",
    "        self.is_bn = is_bn\n",
    "        self.g_iter = g_iter\n",
    "        self.lam = lam\n",
    "        self.sampler = sampler\n",
    "        self.eps = 0.001\n",
    "        self._is_train = False\n",
    "        self.n_hidden = 128 \n",
    "        \n",
    "        if self.trans == 'sparse':\n",
    "            self.data_train, self.data_test, self.scale, self.labels_train, self.labels_test, self.batch_train, self.batch_test, self.batch_info = load_gene_mtx(self.dataset_name, transform=False, count=False, actv=self.gen_activation)\n",
    "            self.N_batch = self.batch_train.shape[1]\n",
    "        else:\n",
    "            self.data_train, self.data_test, self.labels_train, self.labels_val, self.labels_test  = load_gene_mtx(self.dataset_name, transform=True)\n",
    "            self.scale = 1.0\n",
    "                \n",
    "        if self.gen_activation == 'tanh':\n",
    "            self.data = 2* self.data - 1\n",
    "            self.data_train = 2 * self.data_train - 1\n",
    "            self.data_val = 2 * self.data_val - 1\n",
    "\n",
    "        print ('Data set to work on:')\n",
    "        print (self.data_train)\n",
    "        print (self.data_train.shape)\n",
    "        print (self.batch_train)\n",
    "        print (self.batch_train.shape)\n",
    "        \n",
    "        self.train_size = self.data_train.shape[0]\n",
    "        self.test_size = self.data_test.shape[0]\n",
    "        self.total_size = self.test_size\n",
    "\n",
    "        #print(\"Shape self.data_train:\", shape(self.data_train)) \n",
    "        #print(\"Shape self.data_test:\", shape(self.data_test)) \n",
    "    \n",
    "        self.build_model()\n",
    "\n",
    "    build_model = build_model\n",
    "    train_cluster = train_cluster\n",
    "    encoder = encoder\n",
    "    decoder = decoder\n",
    "    discriminatorB = discriminatorB\n",
    "    discriminator2 = discriminator2\n",
    "    discriminator = discriminator\n",
    "    eval_cluster_on_test = eval_cluster_on_test\n",
    "    eval_cluster_on_test_ = eval_cluster_on_test_\n",
    "    zinb_model = zinb_model\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        s = \"DRA_{}_{}_b_{}_g{}_d{}_{}_{}_lr_{}_b1_{}_leak_{}_keep_{}_z_{}_{}_bn_{}_lam_{}_giter_{}_epoch_{}\".format(\n",
    "            datetime.datetime.now(), self.dataset_name, \n",
    "            self.batch_size, self.g_h_dim, self.d_h_dim, self.gen_activation, self.trans, self.lr, \n",
    "            self.beta1, self.leak, self.keep_param, self.z_dim, self.sampler, self.is_bn,\n",
    "            self.lam, self.g_iter, self.epoch) \n",
    "        s = s.replace('[', '_')\n",
    "        s = s.replace(']', '_')\n",
    "        s = s.replace(' ', '')\n",
    "        return s\n",
    "\n",
    "    def sample_Z(self, m, n, sampler='uniform'):\n",
    "        if self.sampler == 'uniform':\n",
    "            return np.random.uniform(-1., 1., size=[m, n])\n",
    "        elif self.sampler == 'normal':\n",
    "            return np.random.randn(m, n)\n",
    "\n",
    "    def next_batch(self, data, batch_info, max_size):\n",
    "\n",
    "        indx = np.random.randint(max_size - self.batch_size)\n",
    "        return data[indx:(indx + self.batch_size), :], batch_info[indx:(indx + self.batch_size), :]\n",
    "        \n",
    "\n",
    "    def next_batch_(self, data, max_size):\n",
    "        #data = data.sample(frac = 1)\n",
    "        indx = np.random.randint(max_size - self.batch_size)\n",
    "        return data[indx:(indx + self.batch_size), :]\n",
    "\n",
    "    def sample_gaussian(self, mean, variance, scope=None):\n",
    "\n",
    "        with tf.variable_scope(scope, 'sample_gaussian'):\n",
    "            sample = tf.random_normal(tf.shape(mean), mean, tf.sqrt(variance))\n",
    "            sample.set_shape(mean.get_shape())\n",
    "            return sample\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBGsSdFINaS7"
   },
   "outputs": [],
   "source": [
    "\n",
    "#python dra.py --model dra --batch_size 128 --learning_rate 0.0007 --beta1 0.9 --n_l 1 \n",
    "#--g_h_l1 512 --d_h_l1 512 --bn False --actv sig --trans sparse --keep 0.9 --leak 0.2 --lam 1.0 --epoch 900 --z_dim 10 --train --dataset Simulation\n",
    "\n",
    "learning_rate = 0.0007 # AJ: 18 may, decrease to address nan issue #0.0007\n",
    "epoch = 300 # \"Epoch to train [25]\") # ideal 200 - 2000 v.bary\n",
    "beta1 = 0.9 # \"Momentum term of adam [0.9]\")\n",
    "batch_size = 128 #\"The size of batch images [128]\")\n",
    "z_dim = 10 # \"Latent space dimension\")\n",
    "n_l = 1 # \"# Hidden Layers\")\n",
    "g_h_l1 = 512 #\"#Generator Hidden Units in Layer 1\")\n",
    "g_h_l2 = 256 # \"#Generator Hidden Units in Layer 2\")\n",
    "g_h_l3 = 0 # \"#Generator Hidden Units in Layer 3\")\n",
    "g_h_l4 = 0# \"#Generator Hidden Units in Layer 4\")\n",
    "d_h_l1 = 512 # \"#Discriminator Hidden Units in Layer 1\")\n",
    "d_h_l2 = 256 # \"#Discriminator Hidden Units in Layer 2\")\n",
    "d_h_l3 = 0 #\"#Discriminator Hidden Units in Layer 3\")\n",
    "d_h_l4 = 0 #Discriminator Hidden Units in Layer 4\")\n",
    "actv = \"sig\" # \"Decoder Activation [sig, tanh, lin]\")\n",
    "leak = 0.2 # \"Leak factor\")\n",
    "keep = 0.9 # \"Keep prob\")\n",
    "trans =  \"sparse\" # \"Data Transformation [dense, sparse]\")\n",
    "dataset = dataset_name # e.g. \"Simulation\" # \"The name of dataset [mnist, 10x_73k, 10x_68k, Zeisel, Macosko]\")\n",
    "checkpoint_dir = \"/data/eugene/AAE-20180306-Hemberg/test_checkpoint\" #\"Directory name to save the checkpoints [checkpoint]\") \n",
    "sample_dir = \"test_samples\" #\"Directory name to save the image samples [samples]\")\n",
    "result_dir = \"test_result\" #\"Directory name to results of gene imputation [result]\")\n",
    "train = True #\"True for training, False for testing [False]\")\n",
    "g_iter = 2 #\"# Generator Iterations [2]\")\n",
    "bn = False #\"True for batch Norm [False]\")\n",
    "lam = 1.0 #\"Lambda for regularization\")\n",
    "sampler = \"normal\" #\"The sampling distribution of z [uniform, normal, mix_gauss]\")\n",
    "model = \"dra\" #\"Model to train [aae, van_ae] [aae]\")\n",
    "X_dim = hvg #\"Input dimension\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QnRxEn0mkQ3J",
    "outputId": "869f1ab2-f66f-49b2-de09-8653928f15fb"
   },
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "4pEabDIFdtka",
    "outputId": "33328811-b833-4745-f9f1-230cb1d476c8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.disable_v2_behavior()\n",
    "tf1.__version__\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Lu41SpbIteg2",
    "outputId": "86852de6-bb35-4000-c294-3adea3be4598",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " \n",
    "run_config = tf.ConfigProto()\n",
    "run_config.gpu_options.per_process_gpu_memory_fraction = 0.333\n",
    "run_config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config = run_config) as sess:\n",
    "\n",
    "    g_h_dim = [g_h_l1, g_h_l2, g_h_l3, g_h_l4]\n",
    "    d_h_dim = [d_h_l1, d_h_l2, d_h_l3, d_h_l4]\n",
    "\n",
    "    if model == 'dra':\n",
    "            test_dra = Test_DRA(\n",
    "                sess,\n",
    "                epoch = epoch,\n",
    "                lr = learning_rate,\n",
    "                beta1 = beta1,\n",
    "                batch_size = batch_size,\n",
    "                X_dim = X_dim, \n",
    "                z_dim = z_dim,\n",
    "                dataset_name = dataset,\n",
    "                checkpoint_dir = checkpoint_dir,\n",
    "                sample_dir = sample_dir,\n",
    "                result_dir = result_dir,\n",
    "                num_layers = n_l,\n",
    "                g_h_dim = g_h_dim[:n_l],\n",
    "                d_h_dim = d_h_dim[:n_l],\n",
    "                gen_activation = actv,\n",
    "                leak = leak,\n",
    "                keep_param = keep,\n",
    "                trans = trans,\n",
    "                is_bn = bn,\n",
    "                g_iter = g_iter,\n",
    "                lam = lam,\n",
    "                sampler = sampler)\n",
    "\n",
    "    # show_all_variables()\n",
    "    if train:\n",
    "        if model == 'dra':\n",
    "            test_dra.train_cluster()           \n",
    "\n",
    "#tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqhlbWlCFp_h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9K2XJ8pcI3A"
   },
   "source": [
    "italicized text"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
